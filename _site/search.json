[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer\n\n\n\n\n\n\nstate space models\n\n\ns4\n\n\nmamba\n\n\nsequence models\n\n\nlong range\n\n\nmodelling\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nCharlene Leong\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Updated - Oldest\n        \n         \n          Updated - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nROS Navigation and Control for a Triangular Holonomic Robot\n\n\n10 min\n\n\nAutonomous SLAM and control\n\n\n\nMay 13, 2024\n\n\n\n\n\nMay 30, 2024\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/crane_plus_motion_planning/index.html",
    "href": "projects/crane_plus_motion_planning/index.html",
    "title": "Crane+ V2 Motion Planning Optimisation",
    "section": "",
    "text": "Open in Github\nThis post describes the experiment setup for testing blackbox optimisation algorithms to motion planning algorithms in the Crane+V2 4DOF robotic arm."
  },
  {
    "objectID": "projects/crane_plus_motion_planning/index.html#build",
    "href": "projects/crane_plus_motion_planning/index.html#build",
    "title": "Crane+ V2 Motion Planning Optimisation",
    "section": "Build",
    "text": "Build\nCrane+V2 is a 4DOF arm + gripper from RT-Net. This project explores the application of blackbox optimisation techniques to optimise hyperparameters in motion planning algorithms in robotic arms. The control state is implemented in ROS with motion planning integration with the MoveIt! Motion Planning Framework and simulation with Gazebo."
  },
  {
    "objectID": "projects/crane_plus_motion_planning/index.html#package-setup",
    "href": "projects/crane_plus_motion_planning/index.html#package-setup",
    "title": "Crane+ V2 Motion Planning Optimisation",
    "section": "Package Setup",
    "text": "Package Setup\nIn order to setup the ROS environment for the Crane+V2 arm, the following packages were implemented to aid in setting up the experimentation environment. Please refer to the repo here for more details on how to things up, even though this is no longer maintained!\ncrane_plus_control: Parameter tuning and benchmarking nodes\ncrane_plus_description: CAD files and URDF (Unified Robot Description Format) model of CRANE+V2\ncrane_plus_gripper: Node that controls the gripper of CRANE+V2\ncrane_plus_hardware: Launch file that configures the settings for use with CRANE+V2 hardware\ncrane_plus_ikfast_arm_plugin: Custom inverse kinematics plugin for CRANE+V2 in the MoveIt! framework\ncrane_plus_joint_state_publisher: Node that converts servo status messages (dynamixel_msgs/JointState message type) output by the Dynamixel servo controller to ROS sensor_msgs/JointState message type\ncrane_plus_moveit_config: Parameters and launch files for using CRANE+V2 with MoveIt! framework\ncrane_plus_simulation: Launch file that configures the settings for simulating CRANE+V2 in Gazebo motion planning"
  },
  {
    "objectID": "projects/crane_plus_motion_planning/index.html#experiment-setup",
    "href": "projects/crane_plus_motion_planning/index.html#experiment-setup",
    "title": "Crane+ V2 Motion Planning Optimisation",
    "section": "Experiment Setup",
    "text": "Experiment Setup\n…\nWIP! Will add details soon!\n…\n\nThis project was part of an internship with the Machine Learning research group at Mitsubishi Electric R&D Department in Kanagawa, Japan 2019. This research was to help to optimise the performance of Mitsubishi Electric’s industrial robotic arm productss."
  },
  {
    "objectID": "projects/bigT_ros_control/index.html",
    "href": "projects/bigT_ros_control/index.html",
    "title": "ROS Navigation and Control for a Triangular Holonomic Robot",
    "section": "",
    "text": "Open in Github\nThis post describes the control node implementation for a three-wheeled omni-wheeled holonomic robot to update motor direction and speed from incoming linear and angular velocities."
  },
  {
    "objectID": "projects/bigT_ros_control/index.html#build",
    "href": "projects/bigT_ros_control/index.html#build",
    "title": "ROS Navigation and Control for a Triangular Holonomic Robot",
    "section": "Build",
    "text": "Build\nBig T is an autonomous three-wheeled omni-wheel holonomic robot with SLAM capabilities.\n\n\n\n\n\nBig T is mounted with both exterioceptive sensors such as a mouse sensor, LIDAR and Intel RealSense and proprioceptive sensors such as IMU and encoders to capture odometry signals to inform the control algorithm."
  },
  {
    "objectID": "projects/bigT_ros_control/index.html#mapping-modes",
    "href": "projects/bigT_ros_control/index.html#mapping-modes",
    "title": "ROS Navigation and Control for a Triangular Holonomic Robot",
    "section": "Mapping Modes",
    "text": "Mapping Modes\nThere are two main mapping modes we can operate in with Big T.\n\nThe map of the environment is first created using Hector SLAM to simulataneously localise and map the environment and save the map. See here for reference. Hector SLAM is used in conjunction with an extended Kalman filter (EKF) to fuse wheel odometry with the IMU to create an improved odometry estimate using the robot_pose_ekf package. This example below shows the robot performing autonomous SLAM to map the environment.\n\n\n\n\n\n\n\nWe can test localising the robot inside the saved map using AMCL (Adaptive Monte Carlo localisation) which estimates 2D position based on particle filter. The robot’s pose is represented as a distribution of particles, where each particle represents a possible pose of the robot. It takes as input a map, LIDAR scans, and transform messages, and outputs an estimated pose. See here for reference."
  },
  {
    "objectID": "projects/bigT_ros_control/index.html#architecture",
    "href": "projects/bigT_ros_control/index.html#architecture",
    "title": "ROS Navigation and Control for a Triangular Holonomic Robot",
    "section": "Architecture",
    "text": "Architecture\nBig T can be run into two operative modes, high level block digrams are shown below:\n\nAutonomous Mode - this mode is often used for SLAM navigation.\n\n\n\n\n\n\n\nTele-operative Mode - this mode is often used for user-controlled navigation in a saved map and also for debugging purposes."
  },
  {
    "objectID": "projects/bigT_ros_control/index.html#control-algorithm",
    "href": "projects/bigT_ros_control/index.html#control-algorithm",
    "title": "ROS Navigation and Control for a Triangular Holonomic Robot",
    "section": "Control Algorithm",
    "text": "Control Algorithm\nThe robot’s velocity commands are published on the cmd_vel topic from either theteleop_twist_keyboard node in teleop mode or move_base node in the ROS navigation stack when in SLAM node. The control node aims to translate directions in the global frame to the individual directions to each motor in three-wheeled omnidirectional robot configuration.\n\n1. Imports and Global Variables\nWe first import our initial C++ ROS headers and instantiate our global variables from the robot’s physical characteristics.\nWe include the following headers - [ros/ros.h] to include too headers necessary to use the most common public pieces of the ROS system. - geometry_msgs/Twist.h to publish linear and angular velocities to facilitate interoperability throughout the system. - geometry_msgs/Quaternion.h to publish orientation in quarternion form. - sensor_msgs/Imu.h to collect sensor data message type from the IMU. The IMU node razor_imu_9dof publishes messages to the \"imu\" topic in the ROS system to be used by motion and planning algorithms such as this robot_pose_ekf. - math.h to perform common mathmetical operations and transformations.\nWe then declare our ROS publishers motor_control_pub to publish the motor control signal to the wheels and imu_pub to publish and update the robot’s orientation.\n\n#include \"ros/ros.h\"\n#include \"geometry_msgs/Twist.h\"\n#include \"geometry_msgs/Quaternion.h\"\n#include \"sensor_msgs/Imu.h\"\n#include &lt;math.h&gt;\n\nfloat omega1, omega2, omega3; // angular velocities\nfloat r = 0.0525; // Wheel radius (105mm/2)\nfloat h = 0.18;   // Distance from the center of the body to the wheel (180mm)\n\nfloat v_x = 0;    // Global translation speed in x (m/s)\nfloat v_y = 0;    // Global translation speed in y (m/s)\nfloat omega_body = 0;  // Rotational speed of the body (rad/s)\nfloat theta = 3.1415;  // Orientation of the body (rad)\n\nros::Publisher motor_control_pub;\nros::Publisher imu_pub;\n\n\n\n\n2. Main Loop\nWe first set up the appropriate ROS publishers and subscribers, to enable ROS signal communication between incoming sensor and output motor nodes in the ROS ecosystem.\nThe main loop is as follows:\n\nInitialises the ROS system with the node name triangle_control.\nCreates ros::NodeHandle to interface with the ROS system.\nSets up publisher pub to publish motor control data on the motor_control_data topic.\nSets up imu_sub to receive IMU data on imu topic to process with imu_callback function. This allows us to explicitly publish the IMU orientation to an orientation topic.\nSets up imu_pub to receive velocity cmmmands on the cmd_vel topic and process with the cmd_vel_callback function. The cmd_vel topic is typically used to publish velocity commands published by the teleop_twist_keyboard node in teleop mode or move_base node in the ROS navigation stack when in SLAM node.\nEnters infinite event loop with ros::spin() to continuously process incoming messages on all topics to which the node is subscribed (eg. imu and cmd_vel), call the appropriate callback functions (eg. imu_callback and cmd_vel_callback) and publish messages to the advertised topics (eg. motor_control_data and orientation). It blocks the main thread and ensures the triangle_control node runs for as long as the ROS system is active.\n\n\nint main(int argc, char **argv){\n\n\tros::init(argc, argv, \"triangle_control\");\n\n\tros::NodeHandle n;\n\n\tmotor_control_pub = n.advertise&lt;geometry_msgs::Twist&gt;(\"motor_control_data\", 1000);\n\n\tros::Subscriber imu_sub = n.subscribe(\"imu\", 1000, imu_callback);\n\n\timu_pub = n.advertise&lt;geometry_msgs::Quaternion&gt;(\"orientation\", 1000);\n\n\tros::Subscriber sub = n.subscribe(\"cmd_vel\", 1000, cmd_vel_callback);\n\n\tros::spin();\n\n\treturn 0;\n\n}\n\n\n\n3: Converting Sensor Input to Angular Velocities\nThe main callback function that updates the robot’s direction is cmd_vel_callback which is subscribed to the cmd_vel topic of a data type of geometry_msgs::Twist which expresses velocity in it’s angular and linear parts. This function translates the desired linear and angular velocities of the robot into individual wheel speeds, converts those speeds to PWM signals, and determines the direction for each wheel. It then publishes these control signals to the motor_control_pub topic to update the robot direction .\n\nvoid cmd_vel_callback(const geometry_msgs::Twist & msg){\n\t/*\n\t\tomega1\t...\trotation speed of motor 1\t(in rad/s)\n\t\tomega2\t...\trotation speed of motor 2\t(in rad/s)\n\t\tomega3\t...\trotation speed of motor 3\t(in rad/s)\n\t*/\n\tROS_INFO(\"Msg received\");\n\n\tgeometry_msgs::Twist out_msg;\n\tv_x = msg.linear.x;\n\tv_y = msg.linear.y;\n\tomega_body = msg.angular.z;\n\n\tomega1 = 1/r * (v_x * cosf(theta) + v_y * sinf(theta) + omega_body * h);\n\tomega2 = 1/r * (cosf(theta) * v_y/3 -  sinf(theta) * v_x/3 +  sqrt(3) * sinf(theta) * v_y/3 +  sqrt(3) * cosf(theta) * v_x /3 + omega_body * h);\n\tomega3 = 1/r * (-sqrt(3) * sinf(theta) * v_y/3 + cosf(theta) * v_y/3 -  sqrt(3)  * cosf(theta) * v_x/3 - sinf(theta) * v_x/3 + omega_body * h);\n\n\t// pwm signal\n\tout_msg.linear.x = omega2pwm(omega1); // convert from rad/s to pwm signal\n\tout_msg.linear.y = omega2pwm(omega2);\n\tout_msg.linear.z = omega2pwm(omega3);\n\n\t// motor direction\n\tout_msg.angular.x = sign(omega1); // set direction bit depending on the rotation speed\n\tout_msg.angular.y = sign(omega2);\n\tout_msg.angular.z = sign(omega3);\n\n\tmotor_control_pub.publish(out_msg);\n}\n\nSee the following sections below for more details.\n\n\n4: Deriving the Kinematic Equations for a Three-Wheeled Omnidirectional Robot\nAssumptions and Setup\n\nRobot Configuration:\n\nThe robot is a triangle with three holonomic wheels.\nThe wheels are positioned 120 degrees apart from each other.\n\nVariables:\n\n\\(v_x\\): Linear velocity in the x-direction (global frame).\n\\(v_y\\): Linear velocity in the y-direction (global frame).\n\\(\\omega_\\text{body}\\): Rotational velocity of the robot around its center.\n\\(\\theta\\): Orientation of the robot (angle between the robot’s frame and the global frame).\n\\(r\\): Radius of each wheel.\n\\(h\\): Distance from the center of the robot to each wheel.\n\n\nDiagram\n\n\n\n\n\nDerivation\nWe aim to derive the angular velocities of the three holonomic wheels \\(\\omega_1\\), \\(\\omega_2\\), \\(\\omega_3\\) based on the robot’s linear velocities (\\(v_x\\), \\(v_y\\)) and rotational velocity (\\(\\omega_{\\text{body}}\\)) in the global frame, considering the robot’s orientation \\(\\theta\\).\n1. Transform Global Velocities to Local Velocities\nFirst, we need to transform The global velocities \\(v_x\\) and \\(v_y\\) into the local frame of the robot using its orientation \\(\\theta\\):\n\\[\nv_{x,\\text{local}} = v_x \\cos(\\theta) + v_y \\sin(\\theta)\n\\]\n\\[\nv_{y,\\text{local}} = -v_x \\sin(\\theta) + v_y \\cos(\\theta)\n\\]\n2. Express Local Velocities and Rotational Velocity in Terms of Wheel Velocities\nEach wheel contributes to the robot’s motion. The linear velocities of the wheels can be decomposed into components that affect the overall motion of the robot.\nFor a wheel positioned at an angle \\(\\alpha\\) with respect to the robot’s frame: \\[\n\\omega_{\\text{wheel}} = \\frac{1}{r} (v_{x,\\text{local}} \\cos(\\alpha) + v_{y,\\text{local}} \\sin(\\alpha) + \\omega_{\\text{body}} h)\n\\]\nWhere \\(\\omega_\\text{wheel}\\) is the angular velocity of the wheel, and \\(h\\) is the distance from the center to the wheel.\n3. Apply to Each Wheel\nEach wheel is positioned \\(120^\\circ\\) apart, so the angles \\(\\alpha\\) for the three wheels are \\(0^\\circ\\), \\(120^\\circ\\), and \\(240^\\circ\\).\nFor Wheel 1 where \\(\\alpha = 0^\\circ\\):\n\\[\n\\omega_1 = \\frac{1}{r} ( v_{x,\\text{local}} \\cos(0) + v_{y,\\text{local}} \\sin(0) + \\omega_{\\text{body}} h )\n\\]\nSince \\(cos(0^\\circ)= 1\\) and \\(\\sin(0^\\circ) = 0\\):\n\\[\n\\omega_1 = \\frac{1}{r} ( v_{x,\\text{local}} + \\omega_{\\text{body}} h )\n\\]\nSubstituting the local velocities:\n\\[\n\\omega_1 = \\frac{1}{r} ( v_x \\cos(\\theta) + v_y \\sin(\\theta) + \\omega_{\\text{body}} h )\n\\]\nFor Wheel 2 where \\(\\alpha = 120^\\circ\\):\n\\[\n\\omega_2 = \\frac{1}{r} ( v_{x,\\text{local}} \\cos(120^\\circ) + v_{y,\\text{local}} \\sin(120^\\circ) + \\omega_{\\text{body}} h)\n\\]\nSince \\(\\cos(120^\\circ) = -\\frac{1}{2}\\) and \\(\\sin(120^\\circ) = \\frac{\\sqrt{3}}{2}\\):\n\\[\n\\omega_2 = \\frac{1}{r} ( v_{x,\\text{local}}( -\\frac{1}{2} ) + v_{y,\\text{local}} ( \\frac{\\sqrt{3}}{2} ) + \\omega_{\\text{body}} h)\n\\]\n\\[\n\\omega_2 = \\frac{1}{r} ( (v_x \\cos(\\theta) + v_y \\sin(\\theta)) ( -\\frac{1}{2} ) + (-v_x \\sin(\\theta) + v_y \\cos(\\theta)) ( \\frac{\\sqrt{3}}{2} ) + \\omega_{\\text{body}} h )\n\\]\nExpanding and simplifying:\n\\[\n\\omega_2 = \\frac{1}{r} ( -\\frac{1}{2} v_x \\cos(\\theta) - \\frac{1}{2} v_y \\sin(\\theta) + \\frac{\\sqrt{3}}{2} (-v_x \\sin(\\theta)) + \\frac{\\sqrt{3}}{2} v_y \\cos(\\theta) + \\omega_{\\text{body}} h )\n\\]\n\\[\n\\omega_2 = \\frac{1}{r} ( -\\frac{1}{2} v_x \\cos(\\theta) - \\frac{1}{2} v_y \\sin(\\theta) - \\frac{\\sqrt{3}}{2} v_x \\sin(\\theta) + \\frac{\\sqrt{3}}{2} v_y \\cos(\\theta) + \\omega_{\\text{body}} h )\n\\]\nCombining terms and simplifying further:\n\\[\n\\omega_2 = \\frac{1}{r} ( \\frac{\\cos(\\theta) v_y}{3} - \\frac{\\sin(\\theta) v_x}{3} + \\frac{\\sqrt{3} \\sin(\\theta) v_y}{3} + \\frac{\\sqrt{3} \\cos(\\theta) v_x}{3}  + \\omega_{\\text{body}} h )\n\\]\nFor Wheel 3 where \\(\\alpha = 240^\\circ\\):\n\\[\n\\omega_3 = \\frac{1}{r} ( v_{x,\\text{local}} \\cos(240^\\circ) + v_{y,\\text{local}} \\sin(240^\\circ) + \\omega_{\\text{body}} h )\n\\]\nSince \\(\\cos(240^\\circ) = -\\frac{1}{2}\\) and \\(\\sin(240^\\circ) = -\\frac{\\sqrt{3}}{2}\\):\n\\[\n\\omega_3 = \\frac{1}{r} ( (v_x \\cos(\\theta) + v_y \\sin(\\theta)) ( -\\frac{1}{2} ) + (-v_x \\sin(\\theta) + v_y \\cos(\\theta)) ( -\\frac{\\sqrt{3}}{2} ) + \\omega_{\\text{body}} h )\n\\]\nExpanding and simplifying:\n\\[\n\\omega_3 = \\frac{1}{r} ( -\\frac{1}{2} v_x \\cos(\\theta) - \\frac{1}{2} v_y \\sin(\\theta) - \\frac{\\sqrt{3}}{2} (-v_x \\sin(\\theta)) - \\frac{\\sqrt{3}}{2} v_y \\cos(\\theta) + \\omega_{\\text{body}} h )\n\\]\n\\[\n\\omega_3 = \\frac{1}{r} ( -\\frac{1}{2} v_x \\cos(\\theta) - \\frac{1}{2} v_y \\sin(\\theta) + \\frac{\\sqrt{3}}{2} v_x \\sin(\\theta) - \\frac{\\sqrt{3}}{2} v_y \\cos(\\theta) + \\omega_{\\text{body}} h )\n\\]\nCombining terms and simplifying further:\n\\[\n\\omega_3 = \\frac{1}{r} ( -\\frac{\\sqrt{3} \\sin(\\theta) v_y}{3} + \\frac{\\cos(\\theta) v_y}{3} - \\frac{\\sqrt{3} \\cos(\\theta) v_x}{3} - \\frac{\\sin(\\theta) v_x}{3} + \\omega_{\\text{body}} h )\n\\]\nFinal Equations\nThe final simplified equations for the angular velocities of the wheels are:\n\\[\n\\omega_1 = \\frac{1}{r} ( v_x \\cos(\\theta) + v_y \\sin(\\theta) + \\omega_{\\text{body}} h )\n\\]\n\\[\n\\omega_2 = \\frac{1}{r} ( \\frac{\\cos(\\theta) v_y}{3} - \\frac{\\sin(\\theta) v_x}{3} + \\frac{\\sqrt{3} \\sin(\\theta) v_y}{3} + \\frac{\\sqrt{3} \\cos(\\theta) v_x}{3}  + \\omega_{\\text{body}} h )\n\\]\n\\[\n\\omega_3 = \\frac{1}{r} ( -\\frac{\\sqrt{3} \\sin(\\theta) v_y}{3} + \\frac{\\cos(\\theta) v_y}{3} - \\frac{\\sqrt{3} \\cos(\\theta) v_x}{3} - \\frac{\\sin(\\theta) v_x}{3} + \\omega_{\\text{body}} * h )\n\\]\nThese equations relate the global motion commands (\\(v_x\\), \\(v_y\\), \\(\\omega_{\\text{body}}\\)) to the angular velocities of the three wheels (\\(\\omega_1\\), \\(\\omega_2\\), \\(\\omega_3\\)) taking into account the robot’s orientation \\(\\theta\\), wheel radius \\(r\\) and distance from center to the wheels \\(h\\).\n\n\n5. Converting angular velocities to PWM signal for motor control\nAfter we’ve calculated the angular velocities in terms on radians per second (\\(rad/s\\)), we have to convert to a Pulse Width Modulation (PWM) signal to send updated speed and direction to the motors.\n1. Angular Velocity to RPM\nWe first convert angular velocity (in radians per second) to revolutions per minute (RPM).\nWe know that \\(1 revolution=2\\pi radians\\) and \\(1 minute = 60 seconds\\), therefore, the conversion factor is:\n\\[\nRPM = \\omega\\times(60/(2\\pi)) \\approx \\omega\\times 9.5493 \\approx \\omega\\times 9.55\n\\]\n2. Conversion from RPM to PWM\nIn order to measure and calibrate the relationship betwen PWM and RPM for the motor, an empirical experiment was performed in order to collect measurements and fit a linear regression model to the data.\nFor series of PWM signals from a low duty cycle to (eg. 10%) to high duty cycle (eg. 90%) generated using an oscilloscope, the respective RPM for the motor was recorded using a tachometer.\nThe resulting linear relationship is:\n\\[\nPWM = a \\times RPM + b  = 2.4307 \\times RPM + 36.2178\n\\]\nThis means that for every unit increase in RPM, the PWM value increases by approximately 2.4307 units, and when the RPM is zero, the PWM value starts at approximately 36.2178.\n3. Handling Very Small Angular Velocities\nSmall angular velocities &lt;=0.05 are ignored, to smooth noise in the signal and avoid unnecessary motor activation.\n\n\tif(fabs(omega) &lt;= 0.05) return 0;\n\n4. Linear Equation of Angular velocity to PWM\nThrough substituting and simplifying the equations above, we can derive the following linear relationship for converting angular velocity to PWM.\n\\[\nPWM = 2.4307 \\times RPM + 36.2178 =  2.43 \\times (\\omega\\times 9.55) + 36.22\n\\]\nThe result is then returned as an integer PWM value which can be used to control motor speed. We later apply this to each wheel when we calculate the new angular velocities from sensor feedback. We make \\(\\omega\\) absolute \\(|\\omega_{\\text{wheel}}|\\) in order to ensure the angular velocity is never non-negative.\n\nint\tomega2pwm(float omega) {\n\t/*\n\t\tomega ... angular velocity ( in rad/s )\n\t\trpm = omega*9.5493; // conversion from rad/s to rpm\t ( 1/(2*pi)*60 = 9.5493 )\n\t\tpwm = 2.4307*rpm + 36.2178; // conversion of rpm to pwm values\n\t*/\n\tif(fabs(omega) &lt;= 0.05) return 0;\n\n\treturn (int)(2.43*(fabs(omega)*9.55) + 36.22);\n}\n\n5. Setting Direction Bit of Motor\nThe direction bit of the motor is set based on the sign of the angular velocity \\(\\omega_{\\text{wheel}}\\).\n\nint sign(float number){\n\tif(number&gt;=0) return 1; else return 0;\n}\n\n\nThis project was executed as part of the course - ECEN430: Advanced Mechatronic Engineering 2: Intelligence and Design at Victoria University of Wellington 2018."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Charlene Leong",
    "section": "",
    "text": "Always endeavouring to understand how AI is transforming our human experience, the technical/non-technical know-how and challenges in effectively bringing R&D to product and client to solve impactful problems.\nCurious and inquisitive, continuously endeavouring to learn, apply and pioneer new uses for machine learning in this ever-growing exciting space. Here are some snippets from my journey and a place to share my learnings!"
  },
  {
    "objectID": "blog/posts/kan/index.html",
    "href": "blog/posts/kan/index.html",
    "title": "KAN",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "blog/posts/mamba/index.html",
    "href": "blog/posts/mamba/index.html",
    "title": "State Space Models Explained: Mamba",
    "section": "",
    "text": "!"
  },
  {
    "objectID": "blog/posts/mamba/index.html#why-state-space-models",
    "href": "blog/posts/mamba/index.html#why-state-space-models",
    "title": "State Space Models Explained: Mamba",
    "section": "Why State Space Models?",
    "text": "Why State Space Models?\nAttention variants struggle to process long sequences efficiently without performance degradation - eg. 100,000k context window++ due to the quadratic nature of attention especially in inference.\nA useful visual explainer can be seen with BERTViz tool [5] in Figure 1 where we can observe attention for one or more attention heads in the same layer as well how individual neurons in the query and key vectors are activated in the attention computation.\n\n\n\n\n\n\n\n\nHead View: Visualising attention head activations between different layers. Connecting lines are weighted based on the attention score between respective words.\n\n\n\n\n\n\n\nNeuron View: Visualisng query, key and value embeddings when computing attention between each token and other tokens within the sequence. Positive values are colored as blue and negative values as orange.\n\n\n\n\n\n\nFigure 1: Visualising Attention Weights in Transformer Networks [5]\n\n\n\nSequence models can be placed on a spectrum based on their approach to information representation, from highly compressed (e.g. RNNs) to highly explicit (e.g. transformers). In theory, state space models behave more like a RNN with"
  },
  {
    "objectID": "blog/posts/mamba/index.html#transformers",
    "href": "blog/posts/mamba/index.html#transformers",
    "title": "Structured State Space Models Explained: Mamba",
    "section": "Transformers",
    "text": "Transformers\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nUnreasonably effective at modelling complex dependencies: Every token gets to explicitly attend to all other prior tokens, instead of relying on a fixed-sized state as a “summary” due to masked attention enabling each token to see an uncompressed view of the sequence when training.\nQuadratic scaling with context length: Since every input attends to all prior inputs, the total amount of computation required accelerates as the number of tokens increases. The cost of inference is therefore quadratic in nature, having to recalculate attention for the full sequence.\n\n\nHighly parallel training: There are no dependencies along the time dimension, and the core operations are matrix multiplications, which hardware accelerators have been excellent at parallelization for decades.\nAutoregressive inference is expensive: Unlike RNNs, there is no fixed-sized compressed representation of the prior tokens; each new token must explicitly attend to all prior tokens."
  },
  {
    "objectID": "blog/posts/mamba/index.html#references",
    "href": "blog/posts/mamba/index.html#references",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "0.4 References",
    "text": "0.4 References\n\n\n[1] Y. Tay et al., “Long range arena: A benchmark for efficient transformers.” 2020. Available: https://arxiv.org/abs/2011.04006\n\n\n[2] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with selective state spaces.” 2023. Available: https://arxiv.org/abs/2312.00752\n\n\n[3] A. Gu, YouTube, 2022. Available: https://www.youtube.com/watch?v=luCBXCErkCs\n\n\n[4] A. Gu, T. Dao, S. Ermon, A. Rudra, and C. Re, “HiPPO: Recurrent memory with optimal polynomial projections.” 2020. Available: https://arxiv.org/abs/2008.07669\n\n\n[5] A. Gu, K. Goel, and C. Ré, “Efficiently modeling long sequences with structured state spaces,” in The international conference on learning representations (ICLR), 2022.\n\n\n[6] OpenAI et al., “GPT-4 technical report.” 2024. Available: https://arxiv.org/abs/2303.08774\n\n\n[7] J. Vig, “A multiscale visualization of attention in the transformer model.” 2019. Available: https://arxiv.org/abs/1906.05714\n\n\n[8] G. Kamradt, “Needle in a haystack - pressure testing LLMs results.” Github, 2023. Available: https://github.com/gkamradt/LLMTest_NeedleInAHaystack\n\n\n[9] A. Gu et al., “Combining recurrent, convolutional, and continuous-time models with linear state-space layers.” 2021. Available: https://arxiv.org/abs/2110.13985\n\n\n[10] M. Grootendorst, “A visual guide to mamba and state space models.” Blog Post, 2024. Available: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state\n\n\n[11] N. Audio, “Fast fourier transformation FFT - basics.” Technical Support, 2024. Available: https://www.nti-audio.com/en/support/know-how/fast-fourier-transform-fft\n\n\n[12] Wikipedia, “Legendre polynomials.” Article, 2024. Available: https://en.wikipedia.org/wiki/Legendre_polynomials\n\n\n[13] A. Gupta, A. Gu, and J. Berant, “Diagonal state spaces are as effective as structured state spaces.” 2022. Available: https://arxiv.org/abs/2203.14343\n\n\n[14] A. Gu, A. Gupta, K. Goel, and C. Ré, “On the parameterization and initialization of diagonal state space models.” 2022. Available: https://arxiv.org/abs/2206.11893\n\n\n\n\n\nFigure 1: Discrete - Continuous Spectrum of Data Sources and Examples\nFigure 2: Long Range Arena: Benchmark Spanning Text Images, Symbolic Reasoning (1K-16K token length) [5]\nFigure 3: Mamba: Matching Transformer Performance with Efficiency in Training and Inference [2]\nFigure 3: Mamba: Matching Transformer Performance with Efficiency in Training and Inference [2]\nHead View: Visualising attention head activations between different layers. Connecting lines are weighted based on the attention score between respective words.\nNeuron View: Visualising query, key and value embeddings when computing attention between each token and other tokens within the sequence. Positive values are colored as blue and negative values as orange.\nOpenAI’s GPT-4-128K\nAnthropic’s Claude 2.1\nFigure 6: Unrolling Recurrent Neural Network Architecture Over Time\nFigure 7: The Three Representations of Linear State Space Layers in S4: (Left) State space models allow us to model continuous-time systems .(Center) The discretised recurrent format can be used for fast autoregressive inference. Recent theory on continuous-time memorisation of the hidden state transition matrix \\(\\mathbf{\\bar{A}}\\) enables us to capture LRDs mathematically and empirically. (Right) Unrolling the RNN into a global convolutional representation allows for efficient training by computing the layer depthwise in parallel. [9]\nFigure 8: Visualising State Space Models [10]\nFigure 8: Visualising State Space Models [10]\nFigure 9: From Continuous to Discrete SSMs With the Zero Order Hold Rule [10]\nFigure 9: From Continuous to Discrete SSMs With the Zero Order Hold Rule [10]\nFigure 10: Visualising 1D Convolution with 1x3 Kernel [11]\nSignal in Time and Frequency Domain [11]\nLegendre Polynomials [12]\nFigure 12: Generalised HIPPO Operator Performing Approximations Over Uniform and Time Varying Measures [3]\nFigure 12: Generalised HIPPO Operator Performing Approximations Over Uniform and Time Varying Measures [3]\nFigure 12: Generalised HIPPO Operator Performing Approximations Over Uniform and Time Varying Measures [3]\nDiagonal Plus Low-rank Approximation\nS4D Recurrent and Convolutional View: Colors denote independent 1D SSMs; purple denotes trainable parameters.\nVisualising S4 vs S4D Results\nS4 vs S4D Long Range Arena Results\nSelective Copying: This requires time-varying models that can selectively remember or ignore inputs depending on their content.\nInduction Heads: This is an associative recall task which requires retrieving an answer based on context, a key ability of LLMs."
  },
  {
    "objectID": "blog/posts/mamba/index.html#rnns",
    "href": "blog/posts/mamba/index.html#rnns",
    "title": "Structured State Space Models Explained: Mamba",
    "section": "RNNs",
    "text": "RNNs"
  },
  {
    "objectID": "projects/posts/bigT_ros_control/index.html",
    "href": "projects/posts/bigT_ros_control/index.html",
    "title": "ROS Navigation and Control for a Triangular Holonomic Robot",
    "section": "",
    "text": "Open in Github\nThis post describes the control node implementation for a three-wheeled omni-wheeled holonomic robot to update motor direction and speed from incoming linear and angular velocities."
  },
  {
    "objectID": "projects/posts/bigT_ros_control/index.html#build",
    "href": "projects/posts/bigT_ros_control/index.html#build",
    "title": "ROS Navigation and Control for a Triangular Holonomic Robot",
    "section": "0.1 Build",
    "text": "0.1 Build\nBig T is an autonomous three-wheeled omni-wheel holonomic robot with SLAM capabilities.\n\n\n\n\n\nBig T is mounted with both exterioceptive sensors such as a mouse sensor, LIDAR and Intel RealSense and proprioceptive sensors such as IMU and encoders to capture odometry signals to inform the control algorithm."
  },
  {
    "objectID": "projects/posts/bigT_ros_control/index.html#mapping-modes",
    "href": "projects/posts/bigT_ros_control/index.html#mapping-modes",
    "title": "ROS Navigation and Control for a Triangular Holonomic Robot",
    "section": "0.2 Mapping Modes",
    "text": "0.2 Mapping Modes\nThere are two main mapping modes we can operate in with Big T.\n\nThe map of the environment is first created using Hector SLAM to simulataneously localise and map the environment and save the map. See here for reference. Hector SLAM is used in conjunction with an extended Kalman filter (EKF) to fuse wheel odometry with the IMU to create an improved odometry estimate using the robot_pose_ekf package. This example below shows the robot performing autonomous SLAM to map the environment.\n\n\n\n\n\n\n\nWe can test localising the robot inside the saved map using AMCL (Adaptive Monte Carlo localisation) which estimates 2D position based on particle filter. The robot’s pose is represented as a distribution of particles, where each particle represents a possible pose of the robot. It takes as input a map, LIDAR scans, and transform messages, and outputs an estimated pose. See here for reference."
  },
  {
    "objectID": "projects/posts/bigT_ros_control/index.html#architecture",
    "href": "projects/posts/bigT_ros_control/index.html#architecture",
    "title": "ROS Navigation and Control for a Triangular Holonomic Robot",
    "section": "0.3 Architecture",
    "text": "0.3 Architecture\nBig T can be run into two operative modes, high level block digrams are shown below:\n\nAutonomous Mode - this mode is often used for SLAM navigation.\n\n\n\n\n\n\n\nTele-operative Mode - this mode is often used for user-controlled navigation in a saved map and also for debugging purposes."
  },
  {
    "objectID": "projects/posts/bigT_ros_control/index.html#control-algorithm",
    "href": "projects/posts/bigT_ros_control/index.html#control-algorithm",
    "title": "ROS Navigation and Control for a Triangular Holonomic Robot",
    "section": "0.4 Control Algorithm",
    "text": "0.4 Control Algorithm\nThe robot’s velocity commands are published on the cmd_vel topic from either theteleop_twist_keyboard node in teleop mode or move_base node in the ROS navigation stack when in SLAM node. The control node aims to translate directions in the global frame to the individual directions to each motor in three-wheeled omnidirectional robot configuration.\n\n0.4.1 1. Imports and Global Variables\nWe first import our initial C++ ROS headers and instantiate our global variables from the robot’s physical characteristics.\nWe include the following headers - [ros/ros.h] to include too headers necessary to use the most common public pieces of the ROS system. - geometry_msgs/Twist.h to publish linear and angular velocities to facilitate interoperability throughout the system. - geometry_msgs/Quaternion.h to publish orientation in quarternion form. - sensor_msgs/Imu.h to collect sensor data message type from the IMU. The IMU node razor_imu_9dof publishes messages to the \"imu\" topic in the ROS system to be used by motion and planning algorithms such as this robot_pose_ekf. - math.h to perform common mathmetical operations and transformations.\nWe then declare our ROS publishers motor_control_pub to publish the motor control signal to the wheels and imu_pub to publish and update the robot’s orientation.\n\n#include \"ros/ros.h\"\n#include \"geometry_msgs/Twist.h\"\n#include \"geometry_msgs/Quaternion.h\"\n#include \"sensor_msgs/Imu.h\"\n#include &lt;math.h&gt;\n\nfloat omega1, omega2, omega3; // angular velocities\nfloat r = 0.0525; // Wheel radius (105mm/2)\nfloat h = 0.18;   // Distance from the center of the body to the wheel (180mm)\n\nfloat v_x = 0;    // Global translation speed in x (m/s)\nfloat v_y = 0;    // Global translation speed in y (m/s)\nfloat omega_body = 0;  // Rotational speed of the body (rad/s)\nfloat theta = 3.1415;  // Orientation of the body (rad)\n\nros::Publisher motor_control_pub;\nros::Publisher imu_pub;\n\n\n\n\n0.4.2 2. Main Loop\nWe first set up the appropriate ROS publishers and subscribers, to enable ROS signal communication between incoming sensor and output motor nodes in the ROS ecosystem.\nThe main loop is as follows:\n\nInitialises the ROS system with the node name triangle_control.\nCreates ros::NodeHandle to interface with the ROS system.\nSets up publisher pub to publish motor control data on the motor_control_data topic.\nSets up imu_sub to receive IMU data on imu topic to process with imu_callback function. This allows us to explicitly publish the IMU orientation to an orientation topic.\nSets up imu_pub to receive velocity cmmmands on the cmd_vel topic and process with the cmd_vel_callback function. The cmd_vel topic is typically used to publish velocity commands published by the teleop_twist_keyboard node in teleop mode or move_base node in the ROS navigation stack when in SLAM node.\nEnters infinite event loop with ros::spin() to continuously process incoming messages on all topics to which the node is subscribed (eg. imu and cmd_vel), call the appropriate callback functions (eg. imu_callback and cmd_vel_callback) and publish messages to the advertised topics (eg. motor_control_data and orientation). It blocks the main thread and ensures the triangle_control node runs for as long as the ROS system is active.\n\n\nint main(int argc, char **argv){\n\n\tros::init(argc, argv, \"triangle_control\");\n\n\tros::NodeHandle n;\n\n\tmotor_control_pub = n.advertise&lt;geometry_msgs::Twist&gt;(\"motor_control_data\", 1000);\n\n\tros::Subscriber imu_sub = n.subscribe(\"imu\", 1000, imu_callback);\n\n\timu_pub = n.advertise&lt;geometry_msgs::Quaternion&gt;(\"orientation\", 1000);\n\n\tros::Subscriber sub = n.subscribe(\"cmd_vel\", 1000, cmd_vel_callback);\n\n\tros::spin();\n\n\treturn 0;\n\n}\n\n\n\n0.4.3 3: Converting Sensor Input to Angular Velocities\nThe main callback function that updates the robot’s direction is cmd_vel_callback which is subscribed to the cmd_vel topic of a data type of geometry_msgs::Twist which expresses velocity in it’s angular and linear parts. This function translates the desired linear and angular velocities of the robot into individual wheel speeds, converts those speeds to PWM signals, and determines the direction for each wheel. It then publishes these control signals to the motor_control_pub topic to update the robot direction .\n\nvoid cmd_vel_callback(const geometry_msgs::Twist & msg){\n\t/*\n\t\tomega1\t...\trotation speed of motor 1\t(in rad/s)\n\t\tomega2\t...\trotation speed of motor 2\t(in rad/s)\n\t\tomega3\t...\trotation speed of motor 3\t(in rad/s)\n\t*/\n\tROS_INFO(\"Msg received\");\n\n\tgeometry_msgs::Twist out_msg;\n\tv_x = msg.linear.x;\n\tv_y = msg.linear.y;\n\tomega_body = msg.angular.z;\n\n\tomega1 = 1/r * (v_x * cosf(theta) + v_y * sinf(theta) + omega_body * h);\n\tomega2 = 1/r * (cosf(theta) * v_y/3 -  sinf(theta) * v_x/3 +  sqrt(3) * sinf(theta) * v_y/3 +  sqrt(3) * cosf(theta) * v_x /3 + omega_body * h);\n\tomega3 = 1/r * (-sqrt(3) * sinf(theta) * v_y/3 + cosf(theta) * v_y/3 -  sqrt(3)  * cosf(theta) * v_x/3 - sinf(theta) * v_x/3 + omega_body * h);\n\n\t// pwm signal\n\tout_msg.linear.x = omega2pwm(omega1); // convert from rad/s to pwm signal\n\tout_msg.linear.y = omega2pwm(omega2);\n\tout_msg.linear.z = omega2pwm(omega3);\n\n\t// motor direction\n\tout_msg.angular.x = sign(omega1); // set direction bit depending on the rotation speed\n\tout_msg.angular.y = sign(omega2);\n\tout_msg.angular.z = sign(omega3);\n\n\tmotor_control_pub.publish(out_msg);\n}\n\nSee the following sections below for more details.\n\n\n0.4.4 4: Deriving the Kinematic Equations for a Three-Wheeled Omnidirectional Robot\nAssumptions and Setup\n\nRobot Configuration:\n\nThe robot is a triangle with three holonomic wheels.\nThe wheels are positioned 120 degrees apart from each other.\n\nVariables:\n\n\\(v_x\\): Linear velocity in the x-direction (global frame).\n\\(v_y\\): Linear velocity in the y-direction (global frame).\n\\(\\omega_\\text{body}\\): Rotational velocity of the robot around its center.\n\\(\\theta\\): Orientation of the robot (angle between the robot’s frame and the global frame).\n\\(r\\): Radius of each wheel.\n\\(h\\): Distance from the center of the robot to each wheel.\n\n\nDiagram\n\n\n\n\n\nDerivation\nWe aim to derive the angular velocities of the three holonomic wheels \\(\\omega_1\\), \\(\\omega_2\\), \\(\\omega_3\\) based on the robot’s linear velocities (\\(v_x\\), \\(v_y\\)) and rotational velocity (\\(\\omega_{\\text{body}}\\)) in the global frame, considering the robot’s orientation \\(\\theta\\).\n1. Transform Global Velocities to Local Velocities\nFirst, we need to transform The global velocities \\(v_x\\) and \\(v_y\\) into the local frame of the robot using its orientation \\(\\theta\\):\n\\[\nv_{x,\\text{local}} = v_x \\cos(\\theta) + v_y \\sin(\\theta)\n\\]\n\\[\nv_{y,\\text{local}} = -v_x \\sin(\\theta) + v_y \\cos(\\theta)\n\\]\n2. Express Local Velocities and Rotational Velocity in Terms of Wheel Velocities\nEach wheel contributes to the robot’s motion. The linear velocities of the wheels can be decomposed into components that affect the overall motion of the robot.\nFor a wheel positioned at an angle \\(\\alpha\\) with respect to the robot’s frame: \\[\n\\omega_{\\text{wheel}} = \\frac{1}{r} (v_{x,\\text{local}} \\cos(\\alpha) + v_{y,\\text{local}} \\sin(\\alpha) + \\omega_{\\text{body}} h)\n\\]\nWhere \\(\\omega_\\text{wheel}\\) is the angular velocity of the wheel, and \\(h\\) is the distance from the center to the wheel.\n3. Apply to Each Wheel\nEach wheel is positioned \\(120^\\circ\\) apart, so the angles \\(\\alpha\\) for the three wheels are \\(0^\\circ\\), \\(120^\\circ\\), and \\(240^\\circ\\).\nFor Wheel 1 where \\(\\alpha = 0^\\circ\\):\n\\[\n\\omega_1 = \\frac{1}{r} ( v_{x,\\text{local}} \\cos(0) + v_{y,\\text{local}} \\sin(0) + \\omega_{\\text{body}} h )\n\\]\nSince \\(cos(0^\\circ)= 1\\) and \\(\\sin(0^\\circ) = 0\\):\n\\[\n\\omega_1 = \\frac{1}{r} ( v_{x,\\text{local}} + \\omega_{\\text{body}} h )\n\\]\nSubstituting the local velocities:\n\\[\n\\omega_1 = \\frac{1}{r} ( v_x \\cos(\\theta) + v_y \\sin(\\theta) + \\omega_{\\text{body}} h )\n\\]\nFor Wheel 2 where \\(\\alpha = 120^\\circ\\):\n\\[\n\\omega_2 = \\frac{1}{r} ( v_{x,\\text{local}} \\cos(120^\\circ) + v_{y,\\text{local}} \\sin(120^\\circ) + \\omega_{\\text{body}} h)\n\\]\nSince \\(\\cos(120^\\circ) = -\\frac{1}{2}\\) and \\(\\sin(120^\\circ) = \\frac{\\sqrt{3}}{2}\\):\n\\[\n\\omega_2 = \\frac{1}{r} ( v_{x,\\text{local}}( -\\frac{1}{2} ) + v_{y,\\text{local}} ( \\frac{\\sqrt{3}}{2} ) + \\omega_{\\text{body}} h)\n\\]\n\\[\n\\omega_2 = \\frac{1}{r} ( (v_x \\cos(\\theta) + v_y \\sin(\\theta)) ( -\\frac{1}{2} ) + (-v_x \\sin(\\theta) + v_y \\cos(\\theta)) ( \\frac{\\sqrt{3}}{2} ) + \\omega_{\\text{body}} h )\n\\]\nExpanding and simplifying:\n\\[\n\\omega_2 = \\frac{1}{r} ( -\\frac{1}{2} v_x \\cos(\\theta) - \\frac{1}{2} v_y \\sin(\\theta) + \\frac{\\sqrt{3}}{2} (-v_x \\sin(\\theta)) + \\frac{\\sqrt{3}}{2} v_y \\cos(\\theta) + \\omega_{\\text{body}} h )\n\\]\n\\[\n\\omega_2 = \\frac{1}{r} ( -\\frac{1}{2} v_x \\cos(\\theta) - \\frac{1}{2} v_y \\sin(\\theta) - \\frac{\\sqrt{3}}{2} v_x \\sin(\\theta) + \\frac{\\sqrt{3}}{2} v_y \\cos(\\theta) + \\omega_{\\text{body}} h )\n\\]\nCombining terms and simplifying further:\n\\[\n\\omega_2 = \\frac{1}{r} ( \\frac{\\cos(\\theta) v_y}{3} - \\frac{\\sin(\\theta) v_x}{3} + \\frac{\\sqrt{3} \\sin(\\theta) v_y}{3} + \\frac{\\sqrt{3} \\cos(\\theta) v_x}{3}  + \\omega_{\\text{body}} h )\n\\]\nFor Wheel 3 where \\(\\alpha = 240^\\circ\\):\n\\[\n\\omega_3 = \\frac{1}{r} ( v_{x,\\text{local}} \\cos(240^\\circ) + v_{y,\\text{local}} \\sin(240^\\circ) + \\omega_{\\text{body}} h )\n\\]\nSince \\(\\cos(240^\\circ) = -\\frac{1}{2}\\) and \\(\\sin(240^\\circ) = -\\frac{\\sqrt{3}}{2}\\):\n\\[\n\\omega_3 = \\frac{1}{r} ( (v_x \\cos(\\theta) + v_y \\sin(\\theta)) ( -\\frac{1}{2} ) + (-v_x \\sin(\\theta) + v_y \\cos(\\theta)) ( -\\frac{\\sqrt{3}}{2} ) + \\omega_{\\text{body}} h )\n\\]\nExpanding and simplifying:\n\\[\n\\omega_3 = \\frac{1}{r} ( -\\frac{1}{2} v_x \\cos(\\theta) - \\frac{1}{2} v_y \\sin(\\theta) - \\frac{\\sqrt{3}}{2} (-v_x \\sin(\\theta)) - \\frac{\\sqrt{3}}{2} v_y \\cos(\\theta) + \\omega_{\\text{body}} h )\n\\]\n\\[\n\\omega_3 = \\frac{1}{r} ( -\\frac{1}{2} v_x \\cos(\\theta) - \\frac{1}{2} v_y \\sin(\\theta) + \\frac{\\sqrt{3}}{2} v_x \\sin(\\theta) - \\frac{\\sqrt{3}}{2} v_y \\cos(\\theta) + \\omega_{\\text{body}} h )\n\\]\nCombining terms and simplifying further:\n\\[\n\\omega_3 = \\frac{1}{r} ( -\\frac{\\sqrt{3} \\sin(\\theta) v_y}{3} + \\frac{\\cos(\\theta) v_y}{3} - \\frac{\\sqrt{3} \\cos(\\theta) v_x}{3} - \\frac{\\sin(\\theta) v_x}{3} + \\omega_{\\text{body}} h )\n\\]\nFinal Equations\nThe final simplified equations for the angular velocities of the wheels are:\n\\[\n\\omega_1 = \\frac{1}{r} ( v_x \\cos(\\theta) + v_y \\sin(\\theta) + \\omega_{\\text{body}} h )\n\\]\n\\[\n\\omega_2 = \\frac{1}{r} ( \\frac{\\cos(\\theta) v_y}{3} - \\frac{\\sin(\\theta) v_x}{3} + \\frac{\\sqrt{3} \\sin(\\theta) v_y}{3} + \\frac{\\sqrt{3} \\cos(\\theta) v_x}{3}  + \\omega_{\\text{body}} h )\n\\]\n\\[\n\\omega_3 = \\frac{1}{r} ( -\\frac{\\sqrt{3} \\sin(\\theta) v_y}{3} + \\frac{\\cos(\\theta) v_y}{3} - \\frac{\\sqrt{3} \\cos(\\theta) v_x}{3} - \\frac{\\sin(\\theta) v_x}{3} + \\omega_{\\text{body}} * h )\n\\]\nThese equations relate the global motion commands (\\(v_x\\), \\(v_y\\), \\(\\omega_{\\text{body}}\\)) to the angular velocities of the three wheels (\\(\\omega_1\\), \\(\\omega_2\\), \\(\\omega_3\\)) taking into account the robot’s orientation \\(\\theta\\), wheel radius \\(r\\) and distance from center to the wheels \\(h\\).\n\n\n0.4.5 5. Converting angular velocities to PWM signal for motor control\nAfter we’ve calculated the angular velocities in terms on radians per second (\\(rad/s\\)), we have to convert to a Pulse Width Modulation (PWM) signal to send updated speed and direction to the motors.\n1. Angular Velocity to RPM\nWe first convert angular velocity (in radians per second) to revolutions per minute (RPM).\nWe know that \\(1 revolution=2\\pi radians\\) and \\(1 minute = 60 seconds\\), therefore, the conversion factor is:\n\\[\nRPM = \\omega\\times(60/(2\\pi)) \\approx \\omega\\times 9.5493 \\approx \\omega\\times 9.55\n\\]\n2. Conversion from RPM to PWM\nIn order to measure and calibrate the relationship betwen PWM and RPM for the motor, an empirical experiment was performed in order to collect measurements and fit a linear regression model to the data.\nFor series of PWM signals from a low duty cycle to (eg. 10%) to high duty cycle (eg. 90%) generated using an oscilloscope, the respective RPM for the motor was recorded using a tachometer.\nThe resulting linear relationship is:\n\\[\nPWM = a \\times RPM + b  = 2.4307 \\times RPM + 36.2178\n\\]\nThis means that for every unit increase in RPM, the PWM value increases by approximately 2.4307 units, and when the RPM is zero, the PWM value starts at approximately 36.2178.\n3. Handling Very Small Angular Velocities\nSmall angular velocities &lt;=0.05 are ignored, to smooth noise in the signal and avoid unnecessary motor activation.\n\n\tif(fabs(omega) &lt;= 0.05) return 0;\n\n4. Linear Equation of Angular velocity to PWM\nThrough substituting and simplifying the equations above, we can derive the following linear relationship for converting angular velocity to PWM.\n\\[\nPWM = 2.4307 \\times RPM + 36.2178 =  2.43 \\times (\\omega\\times 9.55) + 36.22\n\\]\nThe result is then returned as an integer PWM value which can be used to control motor speed. We later apply this to each wheel when we calculate the new angular velocities from sensor feedback. We make \\(\\omega\\) absolute \\(|\\omega_{\\text{wheel}}|\\) in order to ensure the angular velocity is never non-negative.\n\nint\tomega2pwm(float omega) {\n\t/*\n\t\tomega ... angular velocity ( in rad/s )\n\t\trpm = omega*9.5493; // conversion from rad/s to rpm\t ( 1/(2*pi)*60 = 9.5493 )\n\t\tpwm = 2.4307*rpm + 36.2178; // conversion of rpm to pwm values\n\t*/\n\tif(fabs(omega) &lt;= 0.05) return 0;\n\n\treturn (int)(2.43*(fabs(omega)*9.55) + 36.22);\n}\n\n5. Setting Direction Bit of Motor\nThe direction bit of the motor is set based on the sign of the angular velocity \\(\\omega_{\\text{wheel}}\\).\n\nint sign(float number){\n\tif(number&gt;=0) return 1; else return 0;\n}\n\n\nThis project was executed as part of the course - ECEN430: Advanced Mechatronic Engineering 2: Intelligence and Design at Victoria University of Wellington 2018."
  },
  {
    "objectID": "projects/draft/crane_plus_motion_planning/index.html",
    "href": "projects/draft/crane_plus_motion_planning/index.html",
    "title": "Crane+ V2 Motion Planning Optimisation",
    "section": "",
    "text": "Open in Github\nThis post describes the experiment setup for testing blackbox optimisation algorithms to motion planning algorithms in the Crane+V2 4DOF robotic arm."
  },
  {
    "objectID": "projects/draft/crane_plus_motion_planning/index.html#build",
    "href": "projects/draft/crane_plus_motion_planning/index.html#build",
    "title": "Crane+ V2 Motion Planning Optimisation",
    "section": "Build",
    "text": "Build\nCrane+V2 is a 4DOF arm + gripper from RT-Net. This project explores the application of blackbox optimisation techniques to optimise hyperparameters in motion planning algorithms in robotic arms. The control state is implemented in ROS with motion planning integration with the MoveIt! Motion Planning Framework and simulation with Gazebo."
  },
  {
    "objectID": "projects/draft/crane_plus_motion_planning/index.html#package-setup",
    "href": "projects/draft/crane_plus_motion_planning/index.html#package-setup",
    "title": "Crane+ V2 Motion Planning Optimisation",
    "section": "Package Setup",
    "text": "Package Setup\nIn order to setup the ROS environment for the Crane+V2 arm, the following packages were implemented to aid in setting up the experimentation environment. Please refer to the repo here for more details on how to things up, even though this is no longer maintained!\ncrane_plus_control: Parameter tuning and benchmarking nodes\ncrane_plus_description: CAD files and URDF (Unified Robot Description Format) model of CRANE+V2\ncrane_plus_gripper: Node that controls the gripper of CRANE+V2\ncrane_plus_hardware: Launch file that configures the settings for use with CRANE+V2 hardware\ncrane_plus_ikfast_arm_plugin: Custom inverse kinematics plugin for CRANE+V2 in the MoveIt! framework\ncrane_plus_joint_state_publisher: Node that converts servo status messages (dynamixel_msgs/JointState message type) output by the Dynamixel servo controller to ROS sensor_msgs/JointState message type\ncrane_plus_moveit_config: Parameters and launch files for using CRANE+V2 with MoveIt! framework\ncrane_plus_simulation: Launch file that configures the settings for simulating CRANE+V2 in Gazebo motion planning"
  },
  {
    "objectID": "projects/draft/crane_plus_motion_planning/index.html#experiment-setup",
    "href": "projects/draft/crane_plus_motion_planning/index.html#experiment-setup",
    "title": "Crane+ V2 Motion Planning Optimisation",
    "section": "Experiment Setup",
    "text": "Experiment Setup\n…\nWIP! Will add details soon!\n…\n\nThis project was part of an internship with the Machine Learning research group at Mitsubishi Electric R&D Department in Kanagawa, Japan 2019. This research was to help to optimise the performance of Mitsubishi Electric’s industrial robotic arm productss."
  },
  {
    "objectID": "blog/draft/kan/index.html",
    "href": "blog/draft/kan/index.html",
    "title": "KAN",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "blog/posts/mamba/index.html#introduction",
    "href": "blog/posts/mamba/index.html#introduction",
    "title": "Structured State Space Models Explained: Mamba",
    "section": "Introduction",
    "text": "Introduction\nStructured state space models are emerging as a promising alternative in sequence modeling, addressing key limitations of both transformers and recurrent neural networks (RNNs), particularly for modelling long-range dependencies in continuous data such as time series and signal data [1]. Their strengths are suited to problems in the long-range high frame-rate domain such as medical signal, speech and video waveforms. [2]\n\n\n\n\n\n\nLimitations of Transformers for Long Contexts\nIn recent years, transformers have revolutionised AI by enabling models to capture complex relationships within sequences, leading to significant advancements in various fields, including solving the protein folding problem (AlphaFold), performing in the 80-90th percentile in the uniform bar exams and college-level AP subjects[3], to translating between nuanced languages from Tamil, Turkish, Arabic to Urdu. However, transformers face challenges with long sequences (e.g., 100,000 tokens or more) due to the quadratic complexity of the self-attention mechanism, which results in substantial computational and memory costs. This inefficiency becomes particularly problematic during inference.\nA useful visual explainer can be seen with the BERTViz tool [4] in Figure 1 where we can observe attention for one or more attention heads in the same layer as well how individual neurons in the query and key vectors are activated in the attention computation.\n\n\n\n\n\n\n\n\nHead View: Visualising attention head activations between different layers. Connecting lines are weighted based on the attention score between respective words.\n\n\n\n\n\n\n\nNeuron View: Visualising query, key and value embeddings when computing attention between each token and other tokens within the sequence. Positive values are colored as blue and negative values as orange.\n\n\n\n\n\n\nFigure 1: Visualising Attention Weights in Transformer Networks [4]\n\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nUnreasonably effective at modelling complex dependencies: Each token explicitly attends to all other tokens in the sequence. Unlike architectures that rely on a fixed-sized state as a summary, masked attention in transformers enables each token to see an uncompressed view of the sequence during training.\nQuadratic scaling with context length: Since every input attends to all prior inputs, the total amount of computation increases quadratically both in time and space. The cost of inference is therefore quadratic in nature, having to recalculate attention for the full sequence.\n\n\nHighly parallel training: There are no dependencies along the time dimension, and the core operations are matrix multiplications, which hardware accelerators have been excellent at parallelisation for decades.\nWeak Inductive Bias: Unlike CNNs, there is almost no prior knowledge of dependency patterns. For example, position information only comes from absolute/relative positional embeddings.\n\n\n\n\n\nLimitations of RNNs for Long Contexts\nBefore transformers, RNNs were the go-to architecture for sequence modeling. RNNs process sequences iteratively, maintaining a hidden state that captures previous information. However, RNNs suffer from vanishing and exploding gradient problems as the sequence length grows, making it difficult for them to learn long-range dependencies effectively. Additionally, the sequential nature of RNN’s inhibit parallelised training.\n\n\n\n\n\n\nFigure 2: Unrolling Recurrent Neural Network Architecture Over Time\n\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nEfficient autoregressive inference: Since \\(h(t)\\) encapsulates prior inputs, the model only needs to consider a small and constant set of new information for each subsequent input.\nIneffective modeling of complex dependencies: All prior context must be compressed, via static updates, into a fixed amount of bits. Therefore, RNNs often suffer with the vanishing gradient problem with long range sequences.\n\n\nNo limits to context length: There is nothing in the formulation that explicitly constrains the model to a maximal sequence length. Inference scales linearly with sequence length.\nSlow training: Training requires sequential backpropagation through time, making poor utilisation of hardware accelerators, e.g., GPUs. Accelerators have enormous throughput for parallel computation, but are otherwise surprisingly slow at sequential computation."
  },
  {
    "objectID": "blog/posts/mamba/index.html#why-structured-state-space-models",
    "href": "blog/posts/mamba/index.html#why-structured-state-space-models",
    "title": "Structured State Space Models (SSMs) Explained: Mamba",
    "section": "Why Structured State Space Models?",
    "text": "Why Structured State Space Models?\nStructured state space models (SSMs) are emerging as a promising alternative in sequence modeling, addressing key limitations of both transformers and recurrent neural networks (RNNs), particularly for modelling long-range dependencies in continuous data such as time series and signal data [1]. It offers a robust framework for handling long-range dependencies efficiently with up to 5x higher throughput than transformers, linear scaling with sequence length, with performance improvement on real data up to million-length sequences [2]. Their strengths are suited to problems in the long-range high frame-rate domain such as medical signal, speech, video or energy waveforms [3].\n ### Limitations of Transformers for Long Contexts\nIn recent years, transformers have revolutionised AI by enabling models to capture complex relationships within sequences, leading to significant advancements in various fields, including solving the protein folding problem (AlphaFold), performing in the 80-90th percentile in the uniform bar exams and college-level AP subjects[4], to translating between nuanced languages from Tamil, Turkish, Arabic to Urdu. However, transformers face challenges with long sequences (e.g., 100,000 tokens or more) due to the quadratic complexity of the self-attention mechanism, which results in substantial computational and memory costs. This inefficiency becomes particularly problematic during inference.\nA useful visual explainer can be seen with the BERTViz tool [5] (see Figure 1) where we can observe attention for one or more attention heads in the same layer as well how individual neurons in the query and key vectors are activated in the attention computation.\n\n\n\n\n\n\n\n\nHead View: Visualising attention head activations between different layers. Connecting lines are weighted based on the attention score between respective words.\n\n\n\n\n\n\n\nNeuron View: Visualising query, key and value embeddings when computing attention between each token and other tokens within the sequence. Positive values are colored as blue and negative values as orange.\n\n\n\n\n\n\nFigure 1: Visualising Attention Weights in Transformer Networks [5]\n\n\n\nWe can observe in the following experiments in Figure 2 pressure testing LLMs that GPT4’s recall performance started to degrade above 73K tokens where the low recall performance was placed between 7-50% document depth given. However, facts at the beginning of documents were recalled regardless of document length. This also seems to be the case for Anthropic’s Claude 2.1 model.\n\n\n\n\n\n\n\n\nOpenAI’s GPT-4-128K\n\n\n\n\n\n\n\nAnthropic’s Claude 2.1\n\n\n\n\n\n\nFigure 2: Needle In A Haystack - Pressure Testing LLMs Results for Long Context Retrieval [6]\n\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nUnreasonably effective at modelling complex dependencies: Each token explicitly attends to all other tokens in the sequence. Unlike architectures that rely on a fixed-sized state as a summary, masked attention in transformers enables each token to see an uncompressed view of the sequence during training.\nQuadratic scaling with context length: Since every input attends to all prior inputs, the total amount of computation increases quadratically both in time and space. The cost of inference is therefore quadratic in nature, having to recalculate attention for the full sequence.\n\n\nHighly parallel training: There are no dependencies along the time dimension, and the core operations are matrix multiplications, which hardware accelerators have been excellent at parallelisation for decades.\nWeak Inductive Bias: Unlike CNNs, there is almost no prior knowledge of dependency patterns. For example, position information only comes from absolute/relative positional embeddings.\n\n\n\n\nLimitations of RNNs for Long Contexts\nBefore transformers, RNNs were the go-to architecture for sequence modeling where they process sequences iteratively, maintaining a hidden state that captures previous information. However, RNNs suffer from vanishing and exploding gradient problems as the sequence length grows, making it difficult for them to learn long-range dependencies effectively. Additionally, the sequential nature of RNN’s inhibit ability to parallelise training.\n\n\n\n\n\n\nFigure 3: Unrolling Recurrent Neural Network Architecture Over Time\n\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nEfficient autoregressive inference: Since the hidden state \\(h(t)\\) encapsulates prior inputs, the model only needs to consider a small and constant set of new information for each subsequent input.\nIneffective modeling of complex dependencies: All prior context must be compressed, via static updates, into a fixed amount of bits. Therefore, RNNs often suffer with the vanishing gradient problem with long range sequences.\n\n\nNo limits to context length: There is nothing in the formulation that explicitly constrains the model to a maximal sequence length and therefore the state is constant. Inference scales linearly with sequence length.\nSlow training: Training requires sequential backpropagation through time, making poor utilisation of hardware accelerators, e.g., GPUs. Accelerators have enormous throughput for parallel computation, but are otherwise surprisingly slow at sequential computation."
  },
  {
    "objectID": "blog/posts/mamba/index.html#what-are-structured-state-space-models",
    "href": "blog/posts/mamba/index.html#what-are-structured-state-space-models",
    "title": "Structured State Space Models (SSMs) Explained: Mamba",
    "section": "What are Structured State Space Models?",
    "text": "What are Structured State Space Models?\nStructured state space models (SSMs) can be placed on a spectrum based on their approach to information representation, from highly compressed (e.g. RNNs) to highly explicit (e.g. transformers) where their architecture can be interpreted as a combination of recurrent neural networks (RNN), convolutional neural networks (CNN) with roots in state space models and Kalman filters from the control theory and signal processing domains.\nThe core idea that makes SSMs work is the theory of treating memory as an online polynomial function approximation problem where a function \\(f(t): \\mathbb{R} \\rightarrow \\mathbb{R}_{+}\\) can be summarised by storing its optimal coefficients in terms of orthogonal polynomial basis functions, which led to the authors to introducing the HIPPO matrix operator leveraging Legendre polynomials for continuous-time memorisation [6]. This matrix aims to compress the past history into hidden state that has enough information to approximately reconstruct the history in a lower-dimensional state of fixed memory size.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Generalised HIPPO Operator Performing Approximations Over Uniform and Time Varying Measures [3]\n\n\n\nFrom the results, we can see that"
  },
  {
    "objectID": "blog/posts/mamba/index.html#what-are-structured-state-space-sequence-models",
    "href": "blog/posts/mamba/index.html#what-are-structured-state-space-sequence-models",
    "title": "Structured State Space Models (SSMs) Explained: Mamba",
    "section": "What are Structured State Space Sequence Models?",
    "text": "What are Structured State Space Sequence Models?\nStructured state space sequence models (SSMs) can be placed on a spectrum based on their approach to information representation, from highly compressed (e.g. RNNs) to highly explicit (e.g. transformers) where their architecture can be interpreted as a combination of recurrent neural networks (RNN), convolutional neural networks (CNN) with roots in state space models and Kalman filters from the control theory and signal processing domains.\n\nBackground\nThe core idea that makes SSMs work is the theory of treating memory as an online polynomial function approximation problem where a function \\(f(t): \\mathbb{R} \\rightarrow \\mathbb{R}_{+}\\) can be summarised by storing its optimal coefficients in terms of orthogonal polynomial basis functions, which led to the authors, Gu et al, to introducing the HIPPO (high-order polynomial projection operators) matrix operator leveraging Legendre polynomials for continuous-time memorisation [4]. This matrix aims to compress the past history into hidden state that has enough information to approximately reconstruct the history in a lower-dimensional state of fixed memory size.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Generalised HIPPO Operator Performing Approximations Over Uniform and Time Varying Measures [3]\n\n\n\nFollowing this insight, the authors, Gu et al, proposed a new\n\n\n\n\n\n\nFigure 8: Linear State Space Layer [9]"
  },
  {
    "objectID": "blog/posts/mamba/index.html#why-mamba-and-structured-state-space-models",
    "href": "blog/posts/mamba/index.html#why-mamba-and-structured-state-space-models",
    "title": "Structured State Space Models (SSMs) Explained: Mamba",
    "section": "Why Mamba and Structured State Space Models?",
    "text": "Why Mamba and Structured State Space Models?\nThe Mamba model architecture has recently emerged as a promising alternative to deep sequence modeling overcoming key limitations in other model families such as transformers, recurrent neural networks (RNNs), convolutional neural networks (CNNs), particularly for modelling long-range dependencies.\nThey come from the family of structured state space models (SSMs), particularly useful for modelling long-range dependencies in continuous data such as time series and signal data [1]. It offers a robust framework for handling long-range dependencies efficiently with up to 5x higher throughput than transformers, linear scaling with sequence length, with in-context learning performance improvement on real data up to million-length sequences [2]. Therefore, their strengths are suited to problems in the long-range high frame-rate domain such as medical signal, speech, video or energy waveforms [3].\n\n\n\n\n\n\nFigure 1: Discrete - Continuous Spectrum of Data Sources and Examples\n\n\n\nIt primarily does this by framing the modelling problem of learning complex nonlinear interdependencies between inputs and outputs as a discretised signal-to-signal learning problem where we aim to learn the compressed selective memory state between a higher-dimensional input signal and it’s online function reconstruction with the goal to learn the properties of the continuous signal in a compressed discrete space. It borrows ideas from control theory and signal processing, where it is analogous to learning an evolving first-order differential equation (eg. Kalman Filter as a state space model) to capture the input signal’s dynamics whilst employing structured matrices (e.g. HIPPO matrix operator [4], diagonal plus low-rank matrices) to reduce computational complexity and utilising Fast Fourier transforms (FFTs) to further speed up computations.\nThe predecessor to Mamba, the S4 model [5], had already shown promising results in the Long Range Arena [1] even on the Path-X task where the task is to determine whether two points are connected between a flattened sequence of the image which is notable as many other models fail at this task as seen in Figure 2.\n\n\n\n\n\n\nFigure 2: Long Range Arena: Benchmark Spanning Text Images, Symbolic Reasoning (1K-16K token length) [5]\n\n\n\nMamba has also been shown to hold their own against the Transformer++ recipe (eg. PaLM and LLama architectures) with significant improvements in scaling with sequence length and model parameters wih its novel scan algorithm. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modelling, the Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. [2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Mamba: Matching Transformer Performance with Efficiency in Training and Inference [2]\n\n\n\n\nLimitations of Transformers for Long Contexts\nIn recent years, transformers have revolutionised AI by enabling models to capture complex relationships within sequences, leading to significant advancements in various fields, including solving the protein folding problem (AlphaFold), performing in the 80-90th percentile in the uniform bar exams and college-level AP subjects[6], to translating between nuanced languages from Tamil, Turkish, Arabic to Urdu. However, transformers face challenges with long sequences (e.g., 100,000 tokens or more) due to the quadratic complexity of the self-attention mechanism, which results in substantial computational and memory costs. This inefficiency becomes particularly problematic during inference.\nA useful visual explainer can be seen with the BERTViz tool [7] (see Figure 4) where we can observe attention for one or more attention heads in the same layer as well how individual neurons in the query and key vectors are activated in the attention computation.\n\n\n\n\n\n\n\n\nHead View: Visualising attention head activations between different layers. Connecting lines are weighted based on the attention score between respective words.\n\n\n\n\n\n\n\nNeuron View: Visualising query, key and value embeddings when computing attention between each token and other tokens within the sequence. Positive values are colored as blue and negative values as orange.\n\n\n\n\n\n\nFigure 4: Visualising Attention Weights in Transformer Networks [7]\n\n\n\nWe can observe in the following experiments in Figure 5 pressure testing LLMs that GPT4’s recall performance started to degrade above 73K tokens where the low recall performance was placed between 7-50% document depth given. However, facts at the beginning of documents were recalled regardless of document length. This also seems to be the case for Anthropic’s Claude 2.1 model.\n\n\n\n\n\n\n\n\nOpenAI’s GPT-4-128K\n\n\n\n\n\n\n\nAnthropic’s Claude 2.1\n\n\n\n\n\n\nFigure 5: Needle In A Haystack - Pressure Testing LLMs Results for Long Context Retrieval [8]\n\n\n\n\\(T\\): Sequence length \\(d\\): Dimension of the hidden state\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nUnreasonably effective at modelling complex dependencies: Each token explicitly attends to all other tokens in the sequence. Unlike architectures that rely on a fixed-sized state as a summary, masked attention in transformers enables each token to see an uncompressed view of the sequence during training.\nQuadratic scaling with context length: Since every input attends to all prior inputs, the total amount of computation increases quadratically both in time and space - \\(T^2d\\). The cost of inference is therefore quadratic in nature, having to recalculate attention for the full sequence.\n\n\nHighly parallel training: There are no dependencies along the time dimension, and the core operations are matrix multiplications, which hardware accelerators have been excellent at parallelisation for decades.\nWeak Inductive Bias: Unlike CNNs, there is almost no prior knowledge of dependency patterns. For example, position information only comes from absolute/relative positional embeddings.\n\n\n\n\n\nLimitations of RNNs for Long Contexts\nBefore transformers, RNNs were the go-to architecture for sequence modeling where they process sequences iteratively, maintaining a hidden state that captures previous information. However, RNNs suffer from vanishing and exploding gradient problems as the sequence length grows, making it difficult for them to learn long-range dependencies effectively. Additionally, the recurrent nature of RNN’s inhibit ability to parallelise training.\n\n\n\n\n\n\nFigure 6: Unrolling Recurrent Neural Network Architecture Over Time\n\n\n\n\\(T\\): Sequence length \\(d\\): Dimension of the hidden state\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nEfficient autoregressive inference: Since the hidden state \\(h(t)\\) encapsulates prior inputs, the model only needs to consider a small and constant set of new information for each subsequent input.\nIneffective modeling of complex dependencies: All prior context must be compressed, via static updates, into a fixed amount of bits. Therefore, RNNs often suffer with the vanishing gradient problem with long range sequences.\n\n\nNo limits to context length: There is nothing in the formulation that explicitly constrains the model to a maximal sequence length and therefore the state is constant. Inference scales linearly with sequence length.\nSlow training: Training requires sequential backpropagation through time, making poor utilisation of hardware accelerators, e.g., GPUs. In feed-forward propagation and backpropagation, the computation of each state is contingent upon the previous step, therefore the training complexity is \\(O(Td^2)\\).\n\n\n\n\n\nComplexity\nIn summary, the state space models are the only models with linear complexity in both time and space for both training and inference.\n\\(T\\): Sequence length \\(d\\): Dimension of the hidden state\n\n\n\n\n\n\n\n\n\nAspect\nRNNs\nTransformers\nState Space Models (SSMs) (Mamba)\n\n\n\n\nTraining Time Complexity\n\\(O(T \\cdot d^2)\\)\n\\(O(T^2 \\cdot d)\\)\n\\(O(T \\cdot d)\\)\n\n\nTraining Space Complexity\n\\(O(T \\cdot d)\\)\n\\(O(T^2 \\cdot d)\\)\n\\(O(T \\cdot d)\\)\n\n\nInference Time Complexity\n\\(O(T \\cdot d^2)\\)\n\\(O(T^2 \\cdot d)\\)\n\\(O(T \\cdot d)\\)\n\n\nInference Space Complexity\n\\(O(d)\\)\n\\(O(T^2 \\cdot d)\\)\n\\(O(d)\\)"
  },
  {
    "objectID": "blog/posts/mamba/index.html#what-are-state-space-sequence-models",
    "href": "blog/posts/mamba/index.html#what-are-state-space-sequence-models",
    "title": "Structured State Space Models (SSMs) Explained: Mamba",
    "section": "What are State Space Sequence Models?",
    "text": "What are State Space Sequence Models?\nState space sequence models (SSMs) can be placed on a spectrum based on their approach to information representation, from highly compressed (e.g. RNNs) to highly explicit (e.g. transformers) where their architecture can be interpreted as a combination of recurrent neural networks (RNN), convolutional neural networks (CNN) with roots in state space models and Kalman filters from the control theory and signal processing domains.\n\nBackground\nThe core idea that makes SSMs work is the theory of treating memory as an online polynomial function approximation problem where a function \\(f(t): \\mathbb{R} \\rightarrow \\mathbb{R}_{+}\\) can be summarised by storing its optimal coefficients in terms of orthogonal polynomial basis functions, which led to the authors, Gu et al, to introducing the HIPPO (high-order polynomial projection operators) matrix operator leveraging Legendre polynomials for continuous-time memorisation [4]. This matrix aims to compress the past history into hidden state that has enough information to approximately reconstruct the history in a lower-dimensional state of fixed memory size.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Generalised HIPPO Operator Performing Approximations Over Uniform and Time Varying Measures [3]\n\n\n\nThis lead to the introduction of a unified framework for applying state space models to sequence modelling called S4 (structured state space sequence models) by combining recurrent, convolutional and continuous-time models with linear state-space layers [9] with the application of the HIPPO matrix initialisation to effectively model the dynamic state in the system. This enables us to model a implicit continuous-time signal as a discretised recurrent network with the benefit of efficient inference whilst its convolutional representation allows for efficient parallelisable training.\n\n\n\n\n\n\nFigure 8: Linear State Space Layer [9]\n\n\n\nThe method they use to transform the recurrent SSM to its convolutional representation is"
  },
  {
    "objectID": "blog/posts/mamba/index.html#why-mamba-and-structured-state-space-sequence-models",
    "href": "blog/posts/mamba/index.html#why-mamba-and-structured-state-space-sequence-models",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "0.1 Why Mamba and Structured State Space Sequence Models?",
    "text": "0.1 Why Mamba and Structured State Space Sequence Models?\nThe Mamba model architecture has recently emerged as a promising alternative to deep sequence modeling overcoming key limitations in other model families such as S4, transformers, recurrent neural networks (RNNs), convolutional neural networks (CNNs), in performing context-aware reasoning with extreme large context windows, up to 1 million tokens. It overcomes challenges in its predecessor, S4 Section 0.2\nStructured state space models (SSMs) are particularly useful for modelling long-range dependencies in continuous data such as time series and signal data [1]. It offers a robust framework for handling long-range dependencies efficiently with up to 5x higher throughput than transformers, linear scaling with sequence length, with in-context learning performance improvement on real data up to million-length sequences [2]. Therefore, their strengths are suited to problems that require the ability to process long-range dependencies such as high frame-rate medical signals, speech, video or energy waveforms [3]; summarising, generating and perform reasoning on novels, movies, and large data corpuses to processing DNA.\n\n\n\n\n\n\nFigure 1: Discrete - Continuous Spectrum of Data Sources and Examples\n\n\n\nIt primarily does this by framing the modelling problem of learning complex non-linear interdependencies between inputs and outputs as a discretised signal-to-signal learning problem where we aim to learn the compressed selective memory state between a higher-dimensional input signal and it’s online function reconstruction with the goal to learn the properties of the continuous signal in a compressed discrete space. It borrows ideas from control theory and signal processing, where it is analogous to learning an evolving first-order differential equation (eg. Kalman Filter as a state space model) to capture the input signal’s dynamics whilst employing structured matrices (e.g. HIPPO matrix operator [4], diagonal plus low-rank matrices) to reduce computational complexity and utilising Fast Fourier transforms (FFTs) to further speed up computations.\nThe predecessor to Mamba, the S4 model [5], was the first SSM to show promising results in the Long Range Arena [1] even on the Path-X task where the task is to determine whether two points are connected between a flattened sequence of the image which is notable as many other models fail at this task as seen in Figure 2.\n\n\n\n\n\n\nFigure 2: Long Range Arena: Benchmark Spanning Text Images, Symbolic Reasoning (1K-16K token length) [5]\n\n\n\nMamba has also been shown to hold their own against the Transformer++ recipe (eg. PaLM and LLama architectures) with significant improvements in scaling with sequence length and model parameters wih its novel scan algorithm. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modelling, the Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. [2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Mamba: Matching Transformer Performance with Efficiency in Training and Inference [2]\n\n\n\n\n0.1.1 Limitations of Transformers for Long Contexts\nIn recent years, transformers have revolutionised AI by enabling models to capture complex relationships within sequences, leading to significant advancements in various fields, including solving the protein folding problem (AlphaFold), performing in the 80-90th percentile in the uniform bar exams and college-level AP subjects[6], to translating between nuanced languages from Tamil, Turkish, Arabic to Urdu. However, transformers face challenges with long sequences (e.g., 100,000 tokens or more) due to the quadratic complexity of the self-attention mechanism, which results in substantial computational and memory costs. This inefficiency becomes particularly problematic during inference.\nA useful visual explainer can be seen with the BERTViz tool [7] (see Figure 4) where we can observe attention for one or more attention heads in the same layer as well how individual neurons in the query and key vectors are activated in the attention computation.\n\n\n\n\n\n\n\n\nHead View: Visualising attention head activations between different layers. Connecting lines are weighted based on the attention score between respective words.\n\n\n\n\n\n\n\nNeuron View: Visualising query, key and value embeddings when computing attention between each token and other tokens within the sequence. Positive values are colored as blue and negative values as orange.\n\n\n\n\n\n\nFigure 4: Visualising Attention Weights in Transformer Networks [7]\n\n\n\nWe can observe in the following experiments in Figure 5 pressure testing LLMs that GPT4’s recall performance started to degrade above 73K tokens where the low recall performance was placed between 7-50% document depth given. However, facts at the beginning of documents were recalled regardless of document length. This also seems to be the case for Anthropic’s Claude 2.1 model.\n\n\n\n\n\n\n\n\nOpenAI’s GPT-4-128K\n\n\n\n\n\n\n\nAnthropic’s Claude 2.1\n\n\n\n\n\n\nFigure 5: Needle In A Haystack - Pressure Testing LLMs Results for Long Context Retrieval [8]\n\n\n\n\\(T\\): Sequence length \\(d\\): Dimension of the hidden state\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nUnreasonably effective at modelling complex dependencies: Each token explicitly attends to all other tokens in the sequence. Unlike architectures that rely on a fixed-sized state as a summary, masked attention in transformers enables each token to see an uncompressed view of the sequence during training.\nQuadratic scaling with context length: Since every input attends to all prior inputs, the total amount of computation increases quadratically both in time and space - \\(T^2d\\). The cost of inference is therefore quadratic in nature, having to recalculate attention for the full sequence.\n\n\nHighly parallel training: There are no dependencies along the time dimension, and the core operations are matrix multiplications, which hardware accelerators have been excellent at parallelisation for decades.\nWeak inductive bias: Unlike CNNs, there is almost no prior knowledge of dependency patterns. For example, position information only comes from absolute/relative positional embeddings.\n\n\n\n\n\n0.1.2 Limitations of RNNs for Long Contexts\nBefore transformers, RNNs were the go-to architecture for sequence modeling where they process sequences iteratively, maintaining a hidden state that captures previous information. However, RNNs suffer from vanishing and exploding gradient problems as the sequence length grows, making it difficult for them to learn long-range dependencies effectively. Additionally, the recurrent nature of RNN’s inhibit ability to parallelise training.\n\n\n\n\n\n\nFigure 6: Unrolling Recurrent Neural Network Architecture Over Time\n\n\n\n\\(T\\): Sequence length \\(d\\): Dimension of the hidden state\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nEfficient autoregressive inference: Since the hidden state \\(h(t)\\) encapsulates prior inputs, the model only needs to consider a small and constant set of new information for each subsequent input.\nIneffective modeling of complex dependencies: All prior context must be compressed, via static updates, into a fixed amount of bits. Therefore, RNNs often suffer with the vanishing gradient problem with long range sequences.\n\n\nNo limits to context length: There is nothing in the formulation that explicitly constrains the model to a maximal sequence length and therefore the state is constant. Inference scales linearly with sequence length.\nSlow training: Training requires sequential backpropagation through time, making poor utilisation of hardware accelerators, e.g., GPUs. In feed-forward propagation and backpropagation, the computation of each state is contingent upon the previous step, therefore the training complexity is \\(O(Td^2)\\).\n\n\n\n\n\n0.1.3 Complexity\nIn summary, the state space models are the only models with linear complexity in both time and space for both training and inference.\n\\(T\\): Sequence length \\(d\\): Dimension of the hidden state\n\n\n\n\n\n\n\n\n\nAspect\nRNNs\nTransformers\nState Space Models (SSMs) (Mamba)\n\n\n\n\nTraining Time Complexity\n\\(O(T \\cdot d^2)\\)\n\\(O(T^2 \\cdot d)\\)\n\\(O(T \\cdot d)\\)\n\n\nTraining Space Complexity\n\\(O(T \\cdot d)\\)\n\\(O(T^2 \\cdot d)\\)\n\\(O(T \\cdot d)\\)\n\n\nInference Time Complexity\n\\(O(T \\cdot d^2)\\)\n\\(O(T^2 \\cdot d)\\)\n\\(O(T \\cdot d)\\)\n\n\nInference Space Complexity\n\\(O(d)\\)\n\\(O(T^2 \\cdot d)\\)\n\\(O(d)\\)"
  },
  {
    "objectID": "blog/posts/mamba/index.html#what-are-structured-state-space-sequence-models-s4",
    "href": "blog/posts/mamba/index.html#what-are-structured-state-space-sequence-models-s4",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "What are Structured State Space Sequence Models (S4)?",
    "text": "What are Structured State Space Sequence Models (S4)?\nStructured state space sequence models (S4) are introduced as a unified framework for applying SSMs to sequence modelling and can be placed on a spectrum between highly compressed (e.g. RNNs) to highly explicit (e.g. transformers) based on their approach to information representation. Their architecture can be interpreted as a combination of recurrent, convolutional and continuous-time models with linear state-space layers [9] with online memory approximation in the form of the HIPPO matrix operator [4] to effectively approximate sequences with long-range dependencies. This framework allows us to represent the model in three representations; as an implicit continuous-time input signal, as a discretised recurrent network for efficient inference and as a convolutional representation which allows for efficient parallelisable training.\n\n\n\n\n\n\nFigure 7: The Three Representations of Linear State Space Layers in S4: (Left) State space models allow us to model continuous-time systems .(Center) The discretised recurrent format can be used for fast autoregressive inference. Recent theory on continuous-time memorisation of the hidden state transition matrix \\(\\mathbf{\\bar{A}}\\) enables us to capture LRDs mathematically and empirically. (Right) Unrolling the RNN into a global convolutional representation allows for efficient training by computing the layer depthwise in parallel. [9]\n\n\n\n\n1. State Space Models\nTo understand S4, we must first understand their origins from classical SSMs, commonly used to describe state representations of continuous-time systems, mathematically formulated as a set of first-order differential equations. The state of the system is represented by a vector of variables (\\(x(t)\\)), and the dynamics of the system are described by how these state variables change over time (\\(\\mathbf{A}\\)).\nTherefore, at each timestep \\(t\\), we project the input sequence \\(x(t) \\in \\mathbb{R}^{M}\\) to higher-dimensional latent state space representation \\(h(t) \\in \\mathbb{R}^{N}\\) (memory state) to derive the predicted output sequence \\(y(t) \\in \\mathbb{R}^{O}\\).\n\\[\n\\text State \\space equation: \\quad  h'(t) = \\mathbf{A}h(t) + \\mathbf{B}x(t)\n\\tag{1}\\]\n\\[\n\\text Output \\space equation: \\quad y(t) = \\mathbf{C}h(t) +  \\mathbf{D}x(t)\n\\tag{2}\\]\nThe matrices definitions therefore are:\n\n\\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\) the system matrix which describes the system dynamics (otherwise known as the state transition matrix)\n\\(\\mathbf{B} \\in \\mathbb{R}^{N \\times M}\\) the input matrix which describes how inputs affect the state\n\\(\\mathbf{C} \\in \\mathbb{R}^{O \\times N}\\) the output matrix which maps the state to the output\n\\(\\mathbf{D} \\in \\mathbb{R}^{O \\times M}\\) the feedthrough or direction transmission matrix which describes how the input directly influences the output\n\nOften, we consider the the case of a single-input single-output system where \\(O=M=1\\) therefore \\(\\mathbf{D}=0\\), where we omit \\(\\mathbf{D}x(t)\\) by treating it as a skip connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Visualising State Space Models [10]\n\n\n\nThese equations suggest that the SSM exhibits global awareness, as the current output is influenced by all preceding input data. When \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) have constant values, Equation 1 defines a linear time-invariant (LTI) system. Otherwise, it describes a linear time-varying (LTV) system, as in Mamba. LTI systems inherently lack the ability to perceive input content, whereas input-aware LTV systems are designed with this capability. This key distinction enables Mamba to surpass the limitations of S4.\n\n\n2. Discretisation for Training and Inference\nIn order to apply the state space model to deep learning applications for language, audio, image data etc, we must first discretise the system. To achieve this, a timescale (step size) parameter, denoted as \\(\\triangle \\in \\mathbb{R}\\), is introduced to represent the resolution of the input to transform the continuous parameters (\\(\\Delta\\), \\(\\mathbf{A}\\), \\(\\mathbf{B}\\)), into discrete forms (\\(\\mathbf{\\bar{A}}\\) and \\(\\mathbf{\\bar{B}}\\)).\nThere are many discretisation rules that can be applied to transform the parameters, in S4, they use the bilinear method. In Mamba, they apply the zero-order hold rule, we discretise the parameters as follows: \\[\n\\begin{align}\n& \\mathbf{\\bar{A}} = \\exp(\\Delta \\mathbf{A}) \\\\\n& \\mathbf{\\bar{B}} = (\\Delta \\mathbf{A})^{-1} (\\bar{\\mathbf{A}} - \\mathbf{I}) (\\Delta \\mathbf{B}) \\\\\n& \\approx (\\Delta \\mathbf{A})^{-1} (\\Delta \\mathbf{A})(\\Delta \\mathbf{B})  \\\\\n& = \\Delta \\mathbf{B}.\n\\end{align}\n\\tag{3}\\]\nBy holding the input constant over each interval and applying the ZOH rule, we simplify the transformation from continuous-time to discrete-time state space representations. This makes the computation more efficient, especially for systems where \\(\\Delta t\\) is small.\nThus, we transform the continuous signal-to-signal problem \\(x(t)\\rightarrow y(t)\\) to a discrete sequence-to-sequence problem \\(x_k \\rightarrow y_k\\) which can be computed as a linear recurrence similarly to RNNs. This discretised recurrent form is used for efficient autoregressive inference where the inputs are seen one timestep at a time (see Figure 6). In practice, \\(x_k\\) is a feature vector of size \\(\\mathbf{C}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: From Continuous to Discrete SSMs With the Zero Order Hold Rule [10]\n\n\n\nTo acommodate for parallelised training, we can unroll the linear recurrent form to yield a global convolutional representation to Equation 4\n\\[\ny = x * \\mathbf{\\bar{K}}\n\\tag{4}\\]\nwhere \\(\\mathbf{\\bar{K}}=(\\mathbf{C}\\mathbf{\\bar{B}}, \\mathbf{C}\\mathbf{\\bar{A}}\\mathbf{\\bar{B}, ..., \\mathbf{C}\\mathbf{\\bar{A}}}^{T-1}\\mathbf{\\bar{B}})\\) represents the SSM convolutional kernel with length \\(T\\) of the entire sequence. We can do this because \\(\\mathbf{\\bar{A}}\\), \\(\\mathbf{\\bar{B}}\\), and \\(\\mathbf{C}\\) are constant. To compute this efficiently, we apply the discrete convolution theorem which states that the convolution of two sequences can be computed as the inverse FFT of the product of their FFTs, transforming the convolution operation into a multiplication in the frequency domain.\n\n\n\n\n\n\nFigure 10: Visualising 1D Convolution with 1x3 Kernel [11]\n\n\n\n\n\n3. The State Transition Matrix \\(\\mathbf{\\bar{A}}\\)\nThe core idea that makes S4 work is the theory of treating memory as an online polynomial function approximation problem where a function \\(f(t): \\mathbb{R} \\rightarrow \\mathbb{R}_{+}\\) can be summarised by the summation of its optimal coefficients in terms of orthogonal polynomial basis functions. This led to the authors, Gu et al, to introducing the HIPPO (high-order polynomial projection operators) matrix operator [4] applying Legendre polynomials for signal decomposition for continuous-time memorisation. Their orthogonal nature ensures minimal redundancy and interference between different components, leading to stable and efficient representations of sequences with the ability to represent functions between an interval \\([1, -1]\\).\n\n\n\n\n\n\n\n\n\n\n\n\nSignal in Time and Frequency Domain [11]\n\n\n\n\n\n\n\n\n\n\n\nLegendre Polynomials [12]\n\n\n\n\n\n\nFigure 11: Decomposing Signals into Legendre Polynomials\n\n\n\nThis state transition matrix aims to compress the past history into hidden state that has enough information to approximately reconstruct the history in a lower-dimensional state of fixed memory size. We can see in Figure 12 how we can learn the compressed form \\(y(t)\\) of the input signal \\(u(t)\\) as a linear combination of the Legendre polynomials in \\(x(t)\\) (or \\(h(t)\\) from our notation above) by applying the HIPPO matrix as \\(\\mathbf{\\bar{A}}\\) at each timestep.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12: Generalised HIPPO Operator Performing Approximations Over Uniform and Time Varying Measures [3]\n\n\n\nIn order to compute this matrix even more efficiently, since we have a structured matrix with known properties, we can speed up the computation of \\(\\mathbf{\\bar{K}}\\) significantly, and overcome the \\(O(d^2T)\\) computational complexity and \\(O(dT)\\) space complexity in applying \\(\\mathbf{\\bar{A}}\\) for each time step in the sequence.\nThe original S4 approach was to leverage the Diagonal Plus Low-Rank (DPLR) structure in complex space [13] which significantly reduces the space and time complexity as we only need to store and compute the diagonal elements and low-rank components of the dense matrix. It can be expressed as \\(\\mathbf{\\bar{A}}=\\mathbf{\\Lambda}+ \\mathbf{PQ^*}\\) where \\(\\mathbf{\\Lambda}\\) is the diagonal matrix and \\(\\mathbf{PQ}\\) are low-rank matrices (vectors for rank-1 updates). The addition of the low-rank term allows the DPLR matrix to capture more complex relationships in LRD compared to a simple diagonal matrix whilst specialised techniques like the Woodbury identity make operations on DPLR matrices feasible and efficient. This was followed by a paper that showed empirically that just using the diagonal matrix and removing the low-rank portion of the DPLR form of the HIPPO matrix, yielded similar results [13].\nThis work led to S4D used in Mamba [14], further improving the computational effiency and expressiveness of \\(\\mathbf{\\bar{A}}\\) by leveraging the Vandermonde Matrix to compute the diagonal matrix, leveraging the properties of eigenvectors and eigenvalues to efficiently capture more complex relationships between state variables (such as powers and exponentials). This is expressed as \\(\\mathbf{\\bar{A}}=\\mathbf{V \\Lambda V^{-1}}\\) where \\(\\mathbf{\\Lambda}\\) is the diagonal matrix of eigenvalues, \\(\\mathbf{V}\\) is the Vandermonde matrix of eigenvectors and \\(\\mathbf{V^{-1}}\\) is the inverse Vandermonde matrix.\n\n\n\n\n\n\n\n\n\n\n\n\nDiagonal Plus Low-rank Approximation\n\n\n\n\n\n\n\n\n\n\n\nS4D Recurrent and Convolutional View: Colors denote independent 1D SSMs; purple denotes trainable parameters.\n\n\n\n\n\n\nFigure 13: S4 vs S4D Architecture [14]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising S4 vs S4D Results\n\n\n\n\n\n\n\n\n\n\n\nS4 vs S4D Long Range Arena Results\n\n\n\n\n\n\nFigure 14: S4 vs S4D Results [14]"
  },
  {
    "objectID": "blog/posts/mamba/index.html#how-does-mamba-improve-on-s4-to-be-a-potential-alternative-to-transformers",
    "href": "blog/posts/mamba/index.html#how-does-mamba-improve-on-s4-to-be-a-potential-alternative-to-transformers",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "0.3 How does Mamba improve on S4 to be a potential alternative to transformers?",
    "text": "0.3 How does Mamba improve on S4 to be a potential alternative to transformers?\nThe fundamental task of\nThere are two core architectural improvements that Mamba introduces to perform competitively with transformers to overcome S4 weaknesses on tasks that are vital to language modelling and generation enabled by dynamic nature of attention. Namely, the ability to perform context-aware reasoning as \\(\\mathbf{\\Delta}\\), \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) are constant for each input, meaning they cannot “attend” to parts of the sequence or selectively retain information based on the input. This can be illustrated by their inability to perform well on the tasks of selective copying and inductive reasoning (see Figure 15) [2].\n\n\n\n\n\n\n\n\n\n\n\n\nSelective Copying: This requires time-varying models that can selectively remember or ignore inputs depending on their content.\n\n\n\n\n\n\n\n\n\n\n\nInduction Heads: This is an associative recall task which requires retrieving an answer based on context, a key ability of LLMs.\n\n\n\n\n\n\nFigure 15: S4 Inabilities to Do Context Aware Reasoning [10]\n\n\n\nTherefore, the first major contribution is to make the system time-variant eg. make \\(\\mathbf{\\Delta}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) as functions of input \\(x(t)\\). However, this means we have to find an alternative to convolution in Equation 4 to parallelise training. To address this, Mamba introduces the selective scan algorithm, which integrates three classical techniques: kernel fusion, parallel scan, and recomputation."
  },
  {
    "objectID": "blog/posts/mamba/index.html#s4",
    "href": "blog/posts/mamba/index.html#s4",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "0.2 What are Structured State Space Sequence Models (S4)?",
    "text": "0.2 What are Structured State Space Sequence Models (S4)?\nStructured state space sequence models (S4) are introduced as a unified framework for applying SSMs to sequence modelling and can be placed on a spectrum between highly compressed (e.g. RNNs) to highly explicit (e.g. transformers) based on their approach to information representation. Their architecture can be interpreted as a combination of recurrent, convolutional and continuous-time models with linear state-space layers [9] with online memory approximation in the form of the HIPPO matrix operator [4] to effectively approximate sequences with long-range dependencies. This framework allows us to represent the model in three representations; as an implicit continuous-time input signal, as a discretised recurrent network for efficient inference and as a convolutional representation which allows for efficient parallelisable training.\n\n\n\n\n\n\nFigure 7: The Three Representations of Linear State Space Layers in S4: (Left) State space models allow us to model continuous-time systems .(Center) The discretised recurrent format can be used for fast autoregressive inference. Recent theory on continuous-time memorisation of the hidden state transition matrix \\(\\mathbf{\\bar{A}}\\) enables us to capture LRDs mathematically and empirically. (Right) Unrolling the RNN into a global convolutional representation allows for efficient training by computing the layer depthwise in parallel. [9]\n\n\n\n\n0.2.1 1. State Space Models\nTo understand S4, we must first understand their origins from classical SSMs, commonly used to describe state representations of continuous-time systems, mathematically formulated as a set of first-order differential equations. The state of the system is represented by a vector of variables (\\(x(t)\\)), and the dynamics of the system are described by how these state variables change over time (\\(\\mathbf{A}\\)).\nTherefore, at each timestep \\(t\\), we project the input sequence \\(x(t) \\in \\mathbb{R}^{M}\\) to higher-dimensional latent state space representation \\(h(t) \\in \\mathbb{R}^{N}\\) (memory state) to derive the predicted output sequence \\(y(t) \\in \\mathbb{R}^{O}\\).\n\\[\n\\text State \\space equation: \\quad  h'(t) = \\mathbf{A}h(t) + \\mathbf{B}x(t)\n\\tag{1}\\]\n\\[\n\\text Output \\space equation: \\quad y(t) = \\mathbf{C}h(t) +  \\mathbf{D}x(t)\n\\tag{2}\\]\nThe matrices definitions therefore are:\n\n\\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\) the system matrix which describes the system dynamics (otherwise known as the state transition matrix)\n\\(\\mathbf{B} \\in \\mathbb{R}^{N \\times M}\\) the input matrix which describes how inputs affect the state\n\\(\\mathbf{C} \\in \\mathbb{R}^{O \\times N}\\) the output matrix which maps the state to the output\n\\(\\mathbf{D} \\in \\mathbb{R}^{O \\times M}\\) the feedthrough or direction transmission matrix which describes how the input directly influences the output\n\nOften, we consider the the case of a single-input single-output system where \\(O=M=1\\) therefore \\(\\mathbf{D}=0\\), where we omit \\(\\mathbf{D}x(t)\\) by treating it as a skip connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Visualising State Space Models [10]\n\n\n\nThese equations suggest that the SSM exhibits global awareness, as the current output is influenced by all preceding input data. When \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) have constant values, Equation 1 defines a linear time-invariant (LTI) system. Otherwise, it describes a linear time-varying (LTV) system, as in Mamba. LTI systems inherently lack the ability to perceive input content, whereas input-aware LTV systems are designed with this capability. This key distinction enables Mamba to surpass the limitations of S4.\n\n\n0.2.2 2. Discretisation for Training and Inference\nIn order to apply the state space model to deep learning applications for language, audio, image data etc, we must first discretise the system. To achieve this, a timescale (step size) parameter, denoted as \\(\\triangle \\in \\mathbb{R}\\), is introduced to represent the resolution of the input to transform the continuous parameters (\\(\\Delta\\), \\(\\mathbf{A}\\), \\(\\mathbf{B}\\)), into discrete forms (\\(\\mathbf{\\bar{A}}\\) and \\(\\mathbf{\\bar{B}}\\)).\nThere are many discretisation rules that can be applied to transform the parameters, in S4, they use the bilinear method. In Mamba, they apply the zero-order hold rule, we discretise the parameters as follows: \\[\n\\begin{align}\n& \\mathbf{\\bar{A}} = \\exp(\\Delta \\mathbf{A}) \\\\\n& \\mathbf{\\bar{B}} = (\\Delta \\mathbf{A})^{-1} (\\bar{\\mathbf{A}} - \\mathbf{I}) (\\Delta \\mathbf{B}) \\\\\n& \\approx (\\Delta \\mathbf{A})^{-1} (\\Delta \\mathbf{A})(\\Delta \\mathbf{B})  \\\\\n& = \\Delta \\mathbf{B}.\n\\end{align}\n\\tag{3}\\]\nBy holding the input constant over each interval and applying the ZOH rule, we simplify the transformation from continuous-time to discrete-time state space representations. This makes the computation more efficient, especially for systems where \\(\\Delta t\\) is small.\nThus, we transform the continuous signal-to-signal problem \\(x(t)\\rightarrow y(t)\\) to a discrete sequence-to-sequence problem \\(x_k \\rightarrow y_k\\) which can be computed as a linear recurrence similarly to RNNs. This discretised recurrent form is used for efficient autoregressive inference where the inputs are seen one timestep at a time (see Figure 6). In practice, \\(x_k\\) is a feature vector of size \\(\\mathbf{C}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: From Continuous to Discrete SSMs With the Zero Order Hold Rule [10]\n\n\n\nTo acommodate for parallelised training, we can unroll the linear recurrent form to yield a global convolutional representation to Equation 4\n\\[\ny = x * \\mathbf{\\bar{K}}\n\\tag{4}\\]\nwhere \\(\\mathbf{\\bar{K}}=(\\mathbf{C}\\mathbf{\\bar{B}}, \\mathbf{C}\\mathbf{\\bar{A}}\\mathbf{\\bar{B}, ..., \\mathbf{C}\\mathbf{\\bar{A}}}^{T-1}\\mathbf{\\bar{B}})\\) represents the SSM convolutional kernel with length \\(T\\) of the entire sequence. We can do this because \\(\\mathbf{\\bar{A}}\\), \\(\\mathbf{\\bar{B}}\\), and \\(\\mathbf{C}\\) are constant. To compute this efficiently, we apply the discrete convolution theorem which states that the convolution of two sequences can be computed as the inverse FFT of the product of their FFTs, transforming the convolution operation into a multiplication in the frequency domain.\n\n\n\n\n\n\nFigure 10: Visualising 1D Convolution with 1x3 Kernel [11]\n\n\n\n\n\n0.2.3 3. The State Transition Matrix \\(\\mathbf{\\bar{A}}\\)\nThe core idea that makes S4 work is the theory of treating memory as an online polynomial function approximation problem where a function \\(f(t): \\mathbb{R} \\rightarrow \\mathbb{R}_{+}\\) can be summarised by the summation of its optimal coefficients in terms of orthogonal polynomial basis functions. This led to the authors, Gu et al, to introducing the HIPPO (high-order polynomial projection operators) matrix operator [4] applying Legendre polynomials for signal decomposition for continuous-time memorisation. Their orthogonal nature ensures minimal redundancy and interference between different components, leading to stable and efficient representations of sequences with the ability to represent functions between an interval \\([1, -1]\\).\n\n\n\n\n\n\n\n\n\n\n\n\nSignal in Time and Frequency Domain [11]\n\n\n\n\n\n\n\n\n\n\n\nLegendre Polynomials [12]\n\n\n\n\n\n\nFigure 11: Decomposing Signals into Legendre Polynomials\n\n\n\nThis state transition matrix aims to compress the past history into hidden state that has enough information to approximately reconstruct the history in a lower-dimensional state of fixed memory size. We can see in Figure 12 how we can learn the compressed form \\(y(t)\\) of the input signal \\(u(t)\\) as a linear combination of the Legendre polynomials in \\(x(t)\\) (or \\(h(t)\\) from our notation above) by applying the HIPPO matrix as \\(\\mathbf{\\bar{A}}\\) at each timestep.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12: Generalised HIPPO Operator Performing Approximations Over Uniform and Time Varying Measures [3]\n\n\n\nIn order to compute this matrix even more efficiently, since we have a structured matrix with known properties, we can speed up the computation of \\(\\mathbf{\\bar{K}}\\) significantly, and overcome the \\(O(d^2T)\\) computational complexity and \\(O(dT)\\) space complexity in applying \\(\\mathbf{\\bar{A}}\\) for each time step in the sequence.\nThe original S4 approach was to leverage the Diagonal Plus Low-Rank (DPLR) structure in complex space [13] which significantly reduces the space and time complexity as we only need to store and compute the diagonal elements and low-rank components of the dense matrix. It can be expressed as \\(\\mathbf{\\bar{A}}=\\mathbf{\\Lambda}+ \\mathbf{PQ^*}\\) where \\(\\mathbf{\\Lambda}\\) is the diagonal matrix and \\(\\mathbf{PQ}\\) are low-rank matrices (vectors for rank-1 updates). The addition of the low-rank term allows the DPLR matrix to capture more complex relationships in LRD compared to a simple diagonal matrix whilst specialised techniques like the Woodbury identity make operations on DPLR matrices feasible and efficient. This was followed by a paper that showed empirically that just using the diagonal matrix and removing the low-rank portion of the DPLR form of the HIPPO matrix, yielded similar results [13].\nThis work led to S4D used in Mamba [14], further improving the computational effiency and expressiveness of \\(\\mathbf{\\bar{A}}\\) by leveraging the Vandermonde Matrix to compute the diagonal matrix, leveraging the properties of eigenvectors and eigenvalues to efficiently capture more complex relationships between state variables (such as powers and exponentials). This is expressed as \\(\\mathbf{\\bar{A}}=\\mathbf{V \\Lambda V^{-1}}\\) where \\(\\mathbf{\\Lambda}\\) is the diagonal matrix of eigenvalues, \\(\\mathbf{V}\\) is the Vandermonde matrix of eigenvectors and \\(\\mathbf{V^{-1}}\\) is the inverse Vandermonde matrix.\n\n\n\n\n\n\n\n\n\n\n\n\nDiagonal Plus Low-rank Approximation\n\n\n\n\n\n\n\n\n\n\n\nS4D Recurrent and Convolutional View: Colors denote independent 1D SSMs; purple denotes trainable parameters.\n\n\n\n\n\n\nFigure 13: S4 vs S4D Architecture [14]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising S4 vs S4D Results\n\n\n\n\n\n\n\n\n\n\n\nS4 vs S4D Long Range Arena Results\n\n\n\n\n\n\nFigure 14: S4 vs S4D Results [14]"
  },
  {
    "objectID": "blog/posts/mamba/index.html#sec-s4",
    "href": "blog/posts/mamba/index.html#sec-s4",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "0.2 What are Structured State Space Sequence Models (S4)?",
    "text": "0.2 What are Structured State Space Sequence Models (S4)?\nStructured state space sequence models (S4) are introduced as a unified framework for applying SSMs to sequence modelling and can be placed on a spectrum between highly compressed (e.g. RNNs) to highly explicit (e.g. transformers) based on their approach to information representation. Their architecture can be interpreted as a combination of recurrent, convolutional and continuous-time models with linear state-space layers [9] with online memory approximation in the form of the HIPPO matrix operator [4] to effectively approximate sequences with long-range dependencies. This framework allows us to represent the model in three representations; as an implicit continuous-time input signal, as a discretised recurrent network for efficient inference and as a convolutional representation which allows for efficient parallelisable training.\n\n\n\n\n\n\nFigure 7: The Three Representations of Linear State Space Layers in S4: (Left) State space models allow us to model continuous-time systems .(Center) The discretised recurrent format can be used for fast autoregressive inference. Recent theory on continuous-time memorisation of the hidden state transition matrix \\(\\mathbf{\\bar{A}}\\) enables us to capture LRDs mathematically and empirically. (Right) Unrolling the RNN into a global convolutional representation allows for efficient training by computing the layer depthwise in parallel. [9]\n\n\n\n\n0.2.1 1. State Space Models\nTo understand S4, we must first understand their origins from classical SSMs, commonly used to describe state representations of continuous-time systems, mathematically formulated as a set of first-order differential equations. The state of the system is represented by a vector of variables (\\(x(t)\\)), and the dynamics of the system are described by how these state variables change over time (\\(\\mathbf{A}\\)).\nTherefore, at each timestep \\(t\\), we project the input sequence \\(x(t) \\in \\mathbb{R}^{M}\\) to higher-dimensional latent state space representation \\(h(t) \\in \\mathbb{R}^{N}\\) (memory state) to derive the predicted output sequence \\(y(t) \\in \\mathbb{R}^{O}\\).\n\\[\n\\text State \\space equation: \\quad  h'(t) = \\mathbf{A}h(t) + \\mathbf{B}x(t)\n\\tag{1}\\]\n\\[\n\\text Output \\space equation: \\quad y(t) = \\mathbf{C}h(t) +  \\mathbf{D}x(t)\n\\tag{2}\\]\nThe matrices definitions therefore are:\n\n\\(\\mathbf{A} \\in \\mathbb{R}^{N \\times N}\\) the system matrix which describes the system dynamics (otherwise known as the state transition matrix)\n\\(\\mathbf{B} \\in \\mathbb{R}^{N \\times M}\\) the input matrix which describes how inputs affect the state\n\\(\\mathbf{C} \\in \\mathbb{R}^{O \\times N}\\) the output matrix which maps the state to the output\n\\(\\mathbf{D} \\in \\mathbb{R}^{O \\times M}\\) the feedthrough or direction transmission matrix which describes how the input directly influences the output\n\nOften, we consider the the case of a single-input single-output system where \\(O=M=1\\) therefore \\(\\mathbf{D}=0\\), where we omit \\(\\mathbf{D}x(t)\\) by treating it as a skip connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Visualising State Space Models [10]\n\n\n\nThese equations suggest that the SSM exhibits global awareness, as the current output is influenced by all preceding input data. When \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) have constant values, Equation 1 defines a linear time-invariant (LTI) system. Otherwise, it describes a linear time-varying (LTV) system, as in Mamba. LTI systems inherently lack the ability to perceive input content, whereas input-aware LTV systems are designed with this capability. This key distinction enables Mamba to surpass the limitations of S4.\n\n\n0.2.2 2. Discretisation for Training and Inference\nIn order to apply the state space model to deep learning applications for language, audio, image data etc, we must first discretise the system. To achieve this, a timescale (step size) parameter, denoted as \\(\\triangle \\in \\mathbb{R}\\), is introduced to represent the resolution of the input to transform the continuous parameters (\\(\\Delta\\), \\(\\mathbf{A}\\), \\(\\mathbf{B}\\)), into discrete forms (\\(\\mathbf{\\bar{A}}\\) and \\(\\mathbf{\\bar{B}}\\)).\nThere are many discretisation rules that can be applied to transform the parameters, in S4, they use the bilinear method. In Mamba, they apply the zero-order hold rule, we discretise the parameters as follows: \\[\n\\begin{align}\n& \\mathbf{\\bar{A}} = \\exp(\\Delta \\mathbf{A}) \\\\\n& \\mathbf{\\bar{B}} = (\\Delta \\mathbf{A})^{-1} (\\bar{\\mathbf{A}} - \\mathbf{I}) (\\Delta \\mathbf{B}) \\\\\n& \\approx (\\Delta \\mathbf{A})^{-1} (\\Delta \\mathbf{A})(\\Delta \\mathbf{B})  \\\\\n& = \\Delta \\mathbf{B}.\n\\end{align}\n\\tag{3}\\]\nBy holding the input constant over each interval and applying the ZOH rule, we simplify the transformation from continuous-time to discrete-time state space representations. This makes the computation more efficient, especially for systems where \\(\\Delta t\\) is small.\nThus, we transform the continuous signal-to-signal problem \\(x(t)\\rightarrow y(t)\\) to a discrete sequence-to-sequence problem \\(x_k \\rightarrow y_k\\) which can be computed as a linear recurrence similarly to RNNs. This discretised recurrent form is used for efficient autoregressive inference where the inputs are seen one timestep at a time (see Figure 6). In practice, \\(x_k\\) is a feature vector of size \\(\\mathbf{C}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: From Continuous to Discrete SSMs With the Zero Order Hold Rule [10]\n\n\n\nTo acommodate for parallelised training, we can unroll the linear recurrent form to yield a global convolutional representation to Equation 4\n\\[\ny = x * \\mathbf{\\bar{K}}\n\\tag{4}\\]\nwhere \\(\\mathbf{\\bar{K}}=(\\mathbf{C}\\mathbf{\\bar{B}}, \\mathbf{C}\\mathbf{\\bar{A}}\\mathbf{\\bar{B}, ..., \\mathbf{C}\\mathbf{\\bar{A}}}^{T-1}\\mathbf{\\bar{B}})\\) represents the SSM convolutional kernel with length \\(T\\) of the entire sequence. We can do this because \\(\\mathbf{\\bar{A}}\\), \\(\\mathbf{\\bar{B}}\\), and \\(\\mathbf{C}\\) are constant. To compute this efficiently, we apply the discrete convolution theorem which states that the convolution of two sequences can be computed as the inverse FFT of the product of their FFTs, transforming the convolution operation into a multiplication in the frequency domain.\n\n\n\n\n\n\nFigure 10: Visualising 1D Convolution with 1x3 Kernel [11]\n\n\n\n\n\n0.2.3 3. The State Transition Matrix \\(\\mathbf{\\bar{A}}\\)\nThe core idea that makes S4 work is the theory of treating memory as an online polynomial function approximation problem where a function \\(f(t): \\mathbb{R} \\rightarrow \\mathbb{R}_{+}\\) can be summarised by the summation of its optimal coefficients in terms of orthogonal polynomial basis functions. This led to the authors, Gu et al, to introducing the HIPPO (high-order polynomial projection operators) matrix operator [4] applying Legendre polynomials for signal decomposition for continuous-time memorisation. Their orthogonal nature ensures minimal redundancy and interference between different components, leading to stable and efficient representations of sequences with the ability to represent functions between an interval \\([1, -1]\\).\n\n\n\n\n\n\n\n\n\n\n\n\nSignal in Time and Frequency Domain [11]\n\n\n\n\n\n\n\n\n\n\n\nLegendre Polynomials [12]\n\n\n\n\n\n\nFigure 11: Decomposing Signals into Legendre Polynomials\n\n\n\nThis state transition matrix aims to compress the past history into hidden state that has enough information to approximately reconstruct the history in a lower-dimensional state of fixed memory size. We can see in Figure 12 how we can learn the compressed form \\(y(t)\\) of the input signal \\(u(t)\\) as a linear combination of the Legendre polynomials in \\(x(t)\\) (or \\(h(t)\\) from our notation above) by applying the HIPPO matrix as \\(\\mathbf{\\bar{A}}\\) at each timestep.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12: Generalised HIPPO Operator Performing Approximations Over Uniform and Time Varying Measures [3]\n\n\n\nIn order to compute this matrix even more efficiently, since we have a structured matrix with known properties, we can speed up the computation of \\(\\mathbf{\\bar{K}}\\) significantly, and overcome the \\(O(d^2T)\\) computational complexity and \\(O(dT)\\) space complexity in applying \\(\\mathbf{\\bar{A}}\\) for each time step in the sequence.\nThe original S4 approach was to leverage the Diagonal Plus Low-Rank (DPLR) structure in complex space [13] which significantly reduces the space and time complexity as we only need to store and compute the diagonal elements and low-rank components of the dense matrix. It can be expressed as \\(\\mathbf{\\bar{A}}=\\mathbf{\\Lambda}+ \\mathbf{PQ^*}\\) where \\(\\mathbf{\\Lambda}\\) is the diagonal matrix and \\(\\mathbf{PQ}\\) are low-rank matrices (vectors for rank-1 updates). The addition of the low-rank term allows the DPLR matrix to capture more complex relationships in LRD compared to a simple diagonal matrix whilst specialised techniques like the Woodbury identity make operations on DPLR matrices feasible and efficient. This was followed by a paper that showed empirically that just using the diagonal matrix and removing the low-rank portion of the DPLR form of the HIPPO matrix, yielded similar results [13].\nThis work led to S4D used in Mamba [14], further improving the computational effiency and expressiveness of \\(\\mathbf{\\bar{A}}\\) by leveraging the Vandermonde Matrix to compute the diagonal matrix, leveraging the properties of eigenvectors and eigenvalues to efficiently capture more complex relationships between state variables (such as powers and exponentials). This is expressed as \\(\\mathbf{\\bar{A}}=\\mathbf{V \\Lambda V^{-1}}\\) where \\(\\mathbf{\\Lambda}\\) is the diagonal matrix of eigenvalues, \\(\\mathbf{V}\\) is the Vandermonde matrix of eigenvectors and \\(\\mathbf{V^{-1}}\\) is the inverse Vandermonde matrix.\n\n\n\n\n\n\n\n\n\n\n\n\nDiagonal Plus Low-rank Approximation\n\n\n\n\n\n\n\n\n\n\n\nS4D Recurrent and Convolutional View: Colors denote independent 1D SSMs; purple denotes trainable parameters.\n\n\n\n\n\n\nFigure 13: S4 vs S4D Architecture [14]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising S4 vs S4D Results\n\n\n\n\n\n\n\n\n\n\n\nS4 vs S4D Long Range Arena Results\n\n\n\n\n\n\nFigure 14: S4 vs S4D Results [14]"
  },
  {
    "objectID": "blog/posts/mamba/index.html#limitations-of-transformers-for-long-contexts",
    "href": "blog/posts/mamba/index.html#limitations-of-transformers-for-long-contexts",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "1.1 Limitations of Transformers for Long Contexts",
    "text": "1.1 Limitations of Transformers for Long Contexts\nIn recent years, transformers have revolutionised AI by enabling models to capture complex relationships within sequences, leading to significant advancements in various fields, including solving the protein folding problem (AlphaFold), performing in the 80-90th percentile in the uniform bar exams and college-level AP subjects[7], to translating between nuanced languages from Tamil, Turkish, Arabic to Urdu. However, transformers face challenges with long sequences (e.g., 100,000 tokens or more) due to the quadratic complexity of the self-attention mechanism, which results in substantial computational and memory costs. This inefficiency becomes particularly problematic during inference.\nA useful visual explainer can be seen with the BERTViz tool [8] (see Figure 1.5) where we can observe attention for one or more attention heads in the same layer as well how individual neurons in the query and key vectors are activated in the attention computation.\n\n\n\n\n\n\n\n\nHead View: Visualising attention head activations between different layers. Connecting lines are weighted based on the attention score between respective words.\n\n\n\n\n\n\n\nNeuron View: Visualising query, key and value embeddings when computing attention between each token and other tokens within the sequence. Positive values are colored as blue and negative values as orange.\n\n\n\n\n\n\nFigure 1.5: Visualising Attention Weights in Transformer Networks [8]\n\n\n\nWe can observe in the following experiments in Figure 1.6 pressure testing LLMs that GPT4’s recall performance started to degrade above 73K tokens where the low recall performance was placed between 7-50% document depth given. However, facts at the beginning of documents were recalled regardless of document length. This also seems to be the case for Anthropic’s Claude 2.1 model.\n\n\n\n\n\n\n\n\nOpenAI’s GPT-4-128K\n\n\n\n\n\n\n\nAnthropic’s Claude 2.1\n\n\n\n\n\n\nFigure 1.6: Needle In A Haystack - Pressure Testing LLMs Results for Long Context Retrieval [9]\n\n\n\n\\(T\\): Sequence length \\(d\\): Dimension of the hidden state\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nUnreasonably effective at modelling complex dependencies: Each token explicitly attends to all other tokens in the sequence. Unlike architectures that rely on a fixed-sized state as a summary, masked attention in transformers enables each token to see an uncompressed view of the sequence during training.\nQuadratic scaling with context length: Since every input attends to all prior inputs, the total amount of computation increases quadratically both in time and space - \\(T^2d\\). The cost of inference is therefore quadratic in nature, having to recalculate attention for the full sequence.\n\n\nHighly parallel training: There are no dependencies along the time dimension, and the core operations are matrix multiplications, which hardware accelerators have been excellent at parallelisation for decades.\nWeak inductive bias: Unlike CNNs, there is almost no prior knowledge of dependency patterns. For example, position information only comes from absolute/relative positional embeddings."
  },
  {
    "objectID": "blog/posts/mamba/index.html#limitations-of-rnns-for-long-contexts",
    "href": "blog/posts/mamba/index.html#limitations-of-rnns-for-long-contexts",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "1.2 Limitations of RNNs for Long Contexts",
    "text": "1.2 Limitations of RNNs for Long Contexts\nBefore transformers, RNNs were the go-to architecture for sequence modeling where they process sequences iteratively, maintaining a hidden state that captures previous information. The fixed size state \\(h_{t−1}\\)​ represents all prior context in a sequence at time \\(t\\). However, RNNs suffer from vanishing and exploding gradient problems as the sequence length grows, making it difficult for them to learn long-range dependencies effectively. Additionally, the recurrent nature of RNN’s inhibit ability to parallelise training.\n\\[\n\\begin{align}\n& h_t &= \\tanh(W_{hh} h_{t-1} + W_{xh} x_t)  \\\\\n& y_t &= W_{hy} h_t\n\\end{align}\n\\tag{1.3}\\]\n\n\n\n\n\n\nFigure 1.9: Unrolling Recurrent Neural Network Architecture Over Time\n\n\n\n\\(N\\): Sequence length \\(d\\): Model parameters\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nEfficient autoregressive inference: Since the hidden state \\(h(t)\\) encapsulates prior inputs, the model only needs to consider a small and constant set of new information for each subsequent input.\nIneffective modeling of complex dependencies: All prior context must be compressed, via static updates, into a fixed amount of bits. Therefore, RNNs often suffer with the vanishing gradient problem with long range sequences.\n\n\nNo limits to context length: There is nothing in the formulation that explicitly constrains the model to a maximal sequence length and therefore the state is constant. Inference scales linearly with sequence length.\nSlow training: Training requires sequential backpropagation through time, making poor utilisation of hardware accelerators, e.g., GPUs. In feed-forward propagation and backpropagation, the computation of each state is contingent upon the previous step, therefore the training complexity is \\(O(Nd^2)\\)."
  },
  {
    "objectID": "blog/posts/mamba/index.html#complexity",
    "href": "blog/posts/mamba/index.html#complexity",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "1.3 Complexity",
    "text": "1.3 Complexity\nIn summary, the state space models are the only models with linear time and space complexity for both training and inference.\n\\(N\\): Sequence length \\(d\\): Model parameters\n\n\n\n\n\n\n\n\n\nAspect\nRNNs\nTransformers\nState Space Models (SSMs) (Mamba)\n\n\n\n\nTraining Time Complexity\n\\(O(N \\cdot d^2)\\)\n\\(O(N^2 \\cdot d)\\)\n\\(O(N \\cdot d)\\)\n\n\nTraining Space Complexity\n\\(O(N \\cdot d)\\)\n\\(O(N^2 \\cdot d)\\)\n\\(O(N \\cdot d)\\)\n\n\nInference Time Complexity\n\\(O(N \\cdot d^2)\\)\n\\(O(N^2 \\cdot d)\\) without KV cache \\(O(N \\cdot d)\\) with KV cache\n\\(O(N \\cdot d)\\)\n\n\nInference Space Complexity\n\\(O(d)\\)\n\\(O(N^2 \\cdot d)\\) without KV cache \\(O(N \\cdot d)\\) with KV cache\n\\(O(d)\\)"
  },
  {
    "objectID": "blog/posts/mamba/index.html#state-space-models",
    "href": "blog/posts/mamba/index.html#state-space-models",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "2.1 State Space Models",
    "text": "2.1 State Space Models\nTo understand S4, we must first understand their origins from classical SSMs, commonly used to describe state representations of continuous-time systems, mathematically formulated as a set of first-order differential equations. The state of the system is represented by a vector of variables \\(x(t)\\), and the dynamics of the system are described by how these state variables change over time (\\(\\mathbf{A}\\)).\nTherefore, at each timestep \\(t\\), we project the input sequence \\(x(t) \\in \\mathbb{R}^{M}\\) to higher-dimensional latent state space representation \\(h(t) \\in \\mathbb{R}^{D}\\) (memory state) to derive the predicted output sequence \\(y(t) \\in \\mathbb{R}^{O}\\).\n\\[\n\\text State \\space equation: \\quad  h'(t) = \\mathbf{A}h(t) + \\mathbf{B}x(t)\n\\tag{2.1}\\]\n\\[\n\\text Output \\space equation: \\quad y(t) = \\mathbf{C}h(t) +  \\mathbf{D}x(t)\n\\tag{2.2}\\]\nThe matrices definitions therefore are:\n\n\\(\\mathbf{A} \\in \\mathbb{R}^{D \\times D}\\) the system matrix which describes the system dynamics (otherwise known as the state transition matrix)\n\\(\\mathbf{B} \\in \\mathbb{R}^{D \\times M}\\) the input matrix which describes how inputs affect the state\n\\(\\mathbf{C} \\in \\mathbb{R}^{O \\times D}\\) the output matrix which maps the state to the output\n\\(\\mathbf{D} \\in \\mathbb{R}^{O \\times M}\\) the feedthrough or direction transmission matrix which describes how the input directly influences the output\n\nOften, we consider the the case of a single-input single-output system where \\(O=M=1\\) therefore \\(\\mathbf{D}=0\\), where we omit \\(\\mathbf{D}x(t)\\) by treating it as a skip connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Visualising State Space Models [1]\n\n\n\nThese equations suggest that the SSM exhibits global awareness, as the current output is influenced by all preceding input data. When \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) have constant values, Equation 2.1 defines a linear time-invariant (LTI) system. Otherwise, it describes a linear time-varying (LTV) system, as in Mamba. LTI systems inherently lack the ability to perceive input content, whereas input-aware LTV systems are designed with this capability. This key distinction enables Mamba to surpass the limitations of S4."
  },
  {
    "objectID": "blog/posts/mamba/index.html#discretisation-for-training-and-inference",
    "href": "blog/posts/mamba/index.html#discretisation-for-training-and-inference",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "2.2 Discretisation for Training and Inference",
    "text": "2.2 Discretisation for Training and Inference\nIn order to apply the state space model to deep learning applications for language, audio, image data etc, we must first discretise the system. To achieve this, a timescale (step size) parameter, denoted as \\(\\Delta in \\mathbb{R}\\), is introduced to represent the resolution of the input to transform the continuous parameters (\\(\\Delta\\), \\(\\mathbf{A}\\), \\(\\mathbf{B}\\)), into discrete forms (\\(\\mathbf{\\bar{A}}\\) and \\(\\mathbf{\\bar{B}}\\)).\nThere are many discretisation rules that can be applied to transform the parameters, in S4, they use the bilinear method. In Mamba, they apply the zero-order hold rule, we discretise the parameters as follows: \\[\n\\begin{align}\n& \\mathbf{\\bar{A}} = \\exp(\\Delta \\mathbf{A}) \\\\\n& \\mathbf{\\bar{B}} = (\\Delta \\mathbf{A})^{-1} (\\bar{\\mathbf{A}} - \\mathbf{I}) (\\Delta \\mathbf{B}) \\\\\n& \\approx (\\Delta \\mathbf{A})^{-1} (\\Delta \\mathbf{A})(\\Delta \\mathbf{B})  \\\\\n& = \\Delta \\mathbf{B}.\n\\end{align}\n\\tag{2.3}\\]\nThus, we transform the continuous signal-to-signal problem \\(x(t)\\rightarrow y(t)\\) to a discrete sequence-to-sequence problem \\(x_k \\rightarrow y_k\\), by holding the input constant over each interval and applying the ZOH rule, which can be then computed as a linear recurrence similarly to RNNs Equation 1.3. This discretised recurrent form is used for efficient autoregressive inference where the inputs are seen one timestep at a time (see Figure 1.9), especially for systems where \\(\\Delta t\\) is small. In practice, \\(x_k\\) is a feature vector of size \\(\\mathbf{C}\\).\n\n\n\n\n\n\n\n\n\n\n\n\nZero Order Hold Sampling Function\n\n\n\n\n\n\n\n\n\n\n\nDiscrete SSM Diagram [1]\n\n\n\n\n\n\nFigure 2.3: From Continuous to Discrete SSMs With the Zero Order Hold Rule\n\n\n\nTo acommodate for parallelised training, we can unroll the linear recurrent form to yield a global convolutional representation to Equation 2.4\n\\[\ny = x * \\mathbf{\\bar{K}}\n\\tag{2.4}\\]\nwhere \\(\\mathbf{\\bar{K}}=(\\mathbf{C}\\mathbf{\\bar{B}}, \\mathbf{C}\\mathbf{\\bar{A}}\\mathbf{\\bar{B}, ..., \\mathbf{C}\\mathbf{\\bar{A}}}^{T-1}\\mathbf{\\bar{B}})\\) represents the SSM convolutional kernel with length \\(T\\) of the entire sequence. We can do this because \\(\\mathbf{\\bar{A}}\\), \\(\\mathbf{\\bar{B}}\\), and \\(\\mathbf{C}\\) are constant. To compute this efficiently, we apply the discrete convolution theorem trick which states that the convolution of two sequences can be computed as the inverse FFT of the product of their FFTs, transforming the convolution operation into a multiplication in the frequency domain.\n\n\n\n\n\n\nFigure 2.4: Visualising 1D Convolution with 1x3 Kernel [16]"
  },
  {
    "objectID": "blog/posts/mamba/index.html#the-state-transition-matrix-mathbfbara",
    "href": "blog/posts/mamba/index.html#the-state-transition-matrix-mathbfbara",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "2.3 The State Transition Matrix \\(\\mathbf{\\bar{A}}\\)",
    "text": "2.3 The State Transition Matrix \\(\\mathbf{\\bar{A}}\\)\nThe core idea that makes S4 work is the theory of treating memory as an online polynomial function approximation problem where a function \\(f(t): \\mathbb{R} \\rightarrow \\mathbb{R}_{+}\\) can be summarised by the summation of its optimal coefficients in terms of orthogonal polynomial basis functions. This led to the authors, Gu et al, to introducing the HIPPO (high-order polynomial projection operators) matrix operator [5] applying Legendre polynomials for signal decomposition for continuous-time memorisation. Their orthogonal nature ensures minimal redundancy and interference between different components, leading to stable and efficient representations of sequences with the ability to represent functions between an interval \\([1, -1]\\).\n\n\n\n\n\n\n\n\n\n\n\n\nSignal in Time and Frequency Domain [17]\n\n\n\n\n\n\n\n\n\n\n\nLegendre Polynomials [18]\n\n\n\n\n\n\nFigure 2.5: Decomposing Signals into Legendre Polynomials\n\n\n\nThis state transition matrix aims to compress the past history into hidden state that has enough information to approximately reconstruct the history in a lower-dimensional state of fixed memory size. We can see in Figure 2.6 how we can learn the compressed form \\(y(t)\\) of the input signal \\(u(t)\\) as a linear combination of the Legendre polynomials in \\(x(t)\\) (or \\(h(t)\\) from our notation above) by applying the HIPPO matrix as \\(\\mathbf{\\bar{A}}\\) at each timestep.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.6: Generalised HIPPO Operator Performing Approximations Over Uniform and Time Varying Measures [4]\n\n\n\nIn order to compute this matrix even more efficiently, since we have a structured matrix with known properties, we can speed up the computation of \\(\\mathbf{\\bar{K}}\\) significantly, and overcome the \\(O(Td^2)\\) computational complexity and \\(O(Td)\\) space complexity in applying \\(\\mathbf{\\bar{A}}\\) for each time step in the sequence.\nThe original S4 approach was to leverage the Diagonal Plus Low-Rank (DPLR) structure in complex space [19] which significantly reduces the space and time complexity as we only need to store and compute the diagonal elements and low-rank components of the dense matrix. It can be expressed as \\(\\mathbf{\\bar{A}}=\\mathbf{\\Lambda}+ \\mathbf{PQ^*}\\) where \\(\\mathbf{\\Lambda}\\) is the diagonal matrix and \\(\\mathbf{PQ}\\) are low-rank matrices (vectors for rank-1 updates). The addition of the low-rank term allows the DPLR matrix to capture more complex relationships in LRD compared to a simple diagonal matrix whilst specialised techniques like the Woodbury identity make operations on DPLR matrices feasible and efficient. This was followed by a paper that showed empirically that just using the diagonal matrix and removing the low-rank portion of the DPLR form of the HIPPO matrix, yielded similar results [19].\nThis work led to S4D used in Mamba [20], further improving the computational effiency and expressiveness of \\(\\mathbf{\\bar{A}}\\) by leveraging the Vandermonde Matrix to compute the diagonal matrix, leveraging the properties of eigenvectors and eigenvalues to efficiently capture more complex relationships between state variables (such as powers and exponentials). This is expressed as \\(\\mathbf{\\bar{A}}=\\mathbf{V \\Lambda V^{-1}}\\) where \\(\\mathbf{\\Lambda}\\) is the diagonal matrix of eigenvalues, \\(\\mathbf{V}\\) is the Vandermonde matrix of eigenvectors and \\(\\mathbf{V^{-1}}\\) is the inverse Vandermonde matrix.\n\n\n\n\n\n\n\n\n\n\n\n\nDiagonal Plus Low-rank Approximation\n\n\n\n\n\n\n\n\n\n\n\nS4D Recurrent and Convolutional View: Colors denote independent 1D SSMs; purple denotes trainable parameters [20]\n\n\n\n\n\n\nFigure 2.7: S4 vs S4D Architecture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising S4 vs S4D Results\n\n\n\n\n\n\n\n\n\n\n\nS4 vs S4D Long Range Arena Results\n\n\n\n\n\n\nFigure 2.8: S4 vs S4D Results [20]"
  },
  {
    "objectID": "projects/posts/bigT_ros_control/index.html#imports-and-global-variables",
    "href": "projects/posts/bigT_ros_control/index.html#imports-and-global-variables",
    "title": "ROS Navigation and Control for a Triangular Holonomic Robot",
    "section": "4.1 Imports and Global Variables",
    "text": "4.1 Imports and Global Variables\nWe first import our initial C++ ROS headers and instantiate our global variables from the robot’s physical characteristics.\nWe include the following headers:\n\n[ros/ros.h] to include too headers necessary to use the most common public pieces of the ROS system.\ngeometry_msgs/Twist.h to publish linear and angular velocities to facilitate interoperability throughout the system.\ngeometry_msgs/Quaternion.h to publish orientation in quarternion form.\nsensor_msgs/Imu.h to collect sensor data message type from the IMU. The IMU node razor_imu_9dof publishes messages to the \"imu\" topic in the ROS system to be used by motion and planning algorithms such as this robot_pose_ekf.\nmath.h to perform common mathmetical operations and transformations.\n\nWe then declare our ROS publishers motor_control_pub to publish the motor control signal to the wheels and imu_pub to publish and update the robot’s orientation.\n\n#include \"ros/ros.h\"\n#include \"geometry_msgs/Twist.h\"\n#include \"geometry_msgs/Quaternion.h\"\n#include \"sensor_msgs/Imu.h\"\n#include &lt;math.h&gt;\n\nfloat omega1, omega2, omega3; // angular velocities\nfloat r = 0.0525; // Wheel radius (105mm/2)\nfloat h = 0.18;   // Distance from the center of the body to the wheel (180mm)\n\nfloat v_x = 0;    // Global translation speed in x (m/s)\nfloat v_y = 0;    // Global translation speed in y (m/s)\nfloat omega_body = 0;  // Rotational speed of the body (rad/s)\nfloat theta = 3.1415;  // Orientation of the body (rad)\n\nros::Publisher motor_control_pub;\nros::Publisher imu_pub;"
  },
  {
    "objectID": "projects/posts/bigT_ros_control/index.html#main-loop",
    "href": "projects/posts/bigT_ros_control/index.html#main-loop",
    "title": "ROS Navigation and Control for a Triangular Holonomic Robot",
    "section": "4.2 Main Loop",
    "text": "4.2 Main Loop\nWe first set up the appropriate ROS publishers and subscribers, to enable ROS signal communication between incoming sensor and output motor nodes in the ROS ecosystem.\nThe main loop is as follows:\n\nInitialises the ROS system with the node name triangle_control.\nCreates ros::NodeHandle to interface with the ROS system.\nSets up publisher pub to publish motor control data on the motor_control_data topic.\nSets up imu_sub to receive IMU data on imu topic to process with imu_callback function. This allows us to explicitly publish the IMU orientation to an orientation topic.\nSets up imu_pub to receive velocity cmmmands on the cmd_vel topic and process with the cmd_vel_callback function. The cmd_vel topic is typically used to publish velocity commands published by the teleop_twist_keyboard node in teleop mode or move_base node in the ROS navigation stack when in SLAM node.\nEnters infinite event loop with ros::spin() to continuously process incoming messages on all topics to which the node is subscribed (eg. imu and cmd_vel), call the appropriate callback functions (eg. imu_callback and cmd_vel_callback) and publish messages to the advertised topics (eg. motor_control_data and orientation). It blocks the main thread and ensures the triangle_control node runs for as long as the ROS system is active.\n\n\nint main(int argc, char **argv){\n\n\tros::init(argc, argv, \"triangle_control\");\n\n\tros::NodeHandle n;\n\n\tmotor_control_pub = n.advertise&lt;geometry_msgs::Twist&gt;(\"motor_control_data\", 1000);\n\n\tros::Subscriber imu_sub = n.subscribe(\"imu\", 1000, imu_callback);\n\n\timu_pub = n.advertise&lt;geometry_msgs::Quaternion&gt;(\"orientation\", 1000);\n\n\tros::Subscriber sub = n.subscribe(\"cmd_vel\", 1000, cmd_vel_callback);\n\n\tros::spin();\n\n\treturn 0;\n\n}"
  },
  {
    "objectID": "projects/posts/bigT_ros_control/index.html#converting-sensor-input-to-angular-velocities",
    "href": "projects/posts/bigT_ros_control/index.html#converting-sensor-input-to-angular-velocities",
    "title": "ROS Navigation and Control for a Triangular Holonomic Robot",
    "section": "4.3 Converting Sensor Input to Angular Velocities",
    "text": "4.3 Converting Sensor Input to Angular Velocities\nThe main callback function that updates the robot’s direction is cmd_vel_callback which is subscribed to the cmd_vel topic of a data type of geometry_msgs::Twist which expresses velocity in it’s angular and linear parts. This function translates the desired linear and angular velocities of the robot into individual wheel speeds, converts those speeds to PWM signals, and determines the direction for each wheel. It then publishes these control signals to the motor_control_pub topic to update the robot direction .\n\nvoid cmd_vel_callback(const geometry_msgs::Twist & msg){\n\t/*\n\t\tomega1\t...\trotation speed of motor 1\t(in rad/s)\n\t\tomega2\t...\trotation speed of motor 2\t(in rad/s)\n\t\tomega3\t...\trotation speed of motor 3\t(in rad/s)\n\t*/\n\tROS_INFO(\"Msg received\");\n\n\tgeometry_msgs::Twist out_msg;\n\tv_x = msg.linear.x;\n\tv_y = msg.linear.y;\n\tomega_body = msg.angular.z;\n\n\tomega1 = 1/r * (v_x * cosf(theta) + v_y * sinf(theta) + omega_body * h);\n\tomega2 = 1/r * (cosf(theta) * v_y/3 -  sinf(theta) * v_x/3 +  sqrt(3) * sinf(theta) * v_y/3 +  sqrt(3) * cosf(theta) * v_x /3 + omega_body * h);\n\tomega3 = 1/r * (-sqrt(3) * sinf(theta) * v_y/3 + cosf(theta) * v_y/3 -  sqrt(3)  * cosf(theta) * v_x/3 - sinf(theta) * v_x/3 + omega_body * h);\n\n\t// pwm signal\n\tout_msg.linear.x = omega2pwm(omega1); // convert from rad/s to pwm signal\n\tout_msg.linear.y = omega2pwm(omega2);\n\tout_msg.linear.z = omega2pwm(omega3);\n\n\t// motor direction\n\tout_msg.angular.x = sign(omega1); // set direction bit depending on the rotation speed\n\tout_msg.angular.y = sign(omega2);\n\tout_msg.angular.z = sign(omega3);\n\n\tmotor_control_pub.publish(out_msg);\n}\n\nSee the following sections below for more details."
  },
  {
    "objectID": "projects/posts/bigT_ros_control/index.html#deriving-the-kinematic-equations-for-a-three-wheeled-omnidirectional-robot",
    "href": "projects/posts/bigT_ros_control/index.html#deriving-the-kinematic-equations-for-a-three-wheeled-omnidirectional-robot",
    "title": "ROS Navigation and Control for a Triangular Holonomic Robot",
    "section": "4.4 Deriving the Kinematic Equations for a Three-Wheeled Omnidirectional Robot",
    "text": "4.4 Deriving the Kinematic Equations for a Three-Wheeled Omnidirectional Robot\nAssumptions and Setup\n\nRobot Configuration:\n\nThe robot is a triangle with three holonomic wheels.\nThe wheels are positioned 120 degrees apart from each other.\n\nVariables:\n\n\\(v_x\\): Linear velocity in the x-direction (global frame).\n\\(v_y\\): Linear velocity in the y-direction (global frame).\n\\(\\omega_\\text{body}\\): Rotational velocity of the robot around its center.\n\\(\\theta\\): Orientation of the robot (angle between the robot’s frame and the global frame).\n\\(r\\): Radius of each wheel.\n\\(h\\): Distance from the center of the robot to each wheel.\n\n\nDiagram\n\n\n\n\n\nDerivation\nWe aim to derive the angular velocities of the three holonomic wheels \\(\\omega_1\\), \\(\\omega_2\\), \\(\\omega_3\\) based on the robot’s linear velocities (\\(v_x\\), \\(v_y\\)) and rotational velocity (\\(\\omega_{\\text{body}}\\)) in the global frame, considering the robot’s orientation \\(\\theta\\).\n1. Transform Global Velocities to Local Velocities\nFirst, we need to transform The global velocities \\(v_x\\) and \\(v_y\\) into the local frame of the robot using its orientation \\(\\theta\\):\n\\[\nv_{x,\\text{local}} = v_x \\cos(\\theta) + v_y \\sin(\\theta)\n\\]\n\\[\nv_{y,\\text{local}} = -v_x \\sin(\\theta) + v_y \\cos(\\theta)\n\\]\n2. Express Local Velocities and Rotational Velocity in Terms of Wheel Velocities\nEach wheel contributes to the robot’s motion. The linear velocities of the wheels can be decomposed into components that affect the overall motion of the robot.\nFor a wheel positioned at an angle \\(\\alpha\\) with respect to the robot’s frame: \\[\n\\omega_{\\text{wheel}} = \\frac{1}{r} (v_{x,\\text{local}} \\cos(\\alpha) + v_{y,\\text{local}} \\sin(\\alpha) + \\omega_{\\text{body}} h)\n\\]\nWhere \\(\\omega_\\text{wheel}\\) is the angular velocity of the wheel, and \\(h\\) is the distance from the center to the wheel.\n3. Apply to Each Wheel\nEach wheel is positioned \\(120^\\circ\\) apart, so the angles \\(\\alpha\\) for the three wheels are \\(0^\\circ\\), \\(120^\\circ\\), and \\(240^\\circ\\).\nFor Wheel 1 where \\(\\alpha = 0^\\circ\\):\n\\[\n\\omega_1 = \\frac{1}{r} ( v_{x,\\text{local}} \\cos(0) + v_{y,\\text{local}} \\sin(0) + \\omega_{\\text{body}} h )\n\\]\nSince \\(cos(0^\\circ)= 1\\) and \\(\\sin(0^\\circ) = 0\\):\n\\[\n\\omega_1 = \\frac{1}{r} ( v_{x,\\text{local}} + \\omega_{\\text{body}} h )\n\\]\nSubstituting the local velocities:\n\\[\n\\omega_1 = \\frac{1}{r} ( v_x \\cos(\\theta) + v_y \\sin(\\theta) + \\omega_{\\text{body}} h )\n\\]\nFor Wheel 2 where \\(\\alpha = 120^\\circ\\):\n\\[\n\\omega_2 = \\frac{1}{r} ( v_{x,\\text{local}} \\cos(120^\\circ) + v_{y,\\text{local}} \\sin(120^\\circ) + \\omega_{\\text{body}} h)\n\\]\nSince \\(\\cos(120^\\circ) = -\\frac{1}{2}\\) and \\(\\sin(120^\\circ) = \\frac{\\sqrt{3}}{2}\\):\n\\[\n\\omega_2 = \\frac{1}{r} ( v_{x,\\text{local}}( -\\frac{1}{2} ) + v_{y,\\text{local}} ( \\frac{\\sqrt{3}}{2} ) + \\omega_{\\text{body}} h)\n\\]\n\\[\n\\omega_2 = \\frac{1}{r} ( (v_x \\cos(\\theta) + v_y \\sin(\\theta)) ( -\\frac{1}{2} ) + (-v_x \\sin(\\theta) + v_y \\cos(\\theta)) ( \\frac{\\sqrt{3}}{2} ) + \\omega_{\\text{body}} h )\n\\]\nExpanding and simplifying:\n\\[\n\\omega_2 = \\frac{1}{r} ( -\\frac{1}{2} v_x \\cos(\\theta) - \\frac{1}{2} v_y \\sin(\\theta) + \\frac{\\sqrt{3}}{2} (-v_x \\sin(\\theta)) + \\frac{\\sqrt{3}}{2} v_y \\cos(\\theta) + \\omega_{\\text{body}} h )\n\\]\n\\[\n\\omega_2 = \\frac{1}{r} ( -\\frac{1}{2} v_x \\cos(\\theta) - \\frac{1}{2} v_y \\sin(\\theta) - \\frac{\\sqrt{3}}{2} v_x \\sin(\\theta) + \\frac{\\sqrt{3}}{2} v_y \\cos(\\theta) + \\omega_{\\text{body}} h )\n\\]\nCombining terms and simplifying further:\n\\[\n\\omega_2 = \\frac{1}{r} ( \\frac{\\cos(\\theta) v_y}{3} - \\frac{\\sin(\\theta) v_x}{3} + \\frac{\\sqrt{3} \\sin(\\theta) v_y}{3} + \\frac{\\sqrt{3} \\cos(\\theta) v_x}{3}  + \\omega_{\\text{body}} h )\n\\]\nFor Wheel 3 where \\(\\alpha = 240^\\circ\\):\n\\[\n\\omega_3 = \\frac{1}{r} ( v_{x,\\text{local}} \\cos(240^\\circ) + v_{y,\\text{local}} \\sin(240^\\circ) + \\omega_{\\text{body}} h )\n\\]\nSince \\(\\cos(240^\\circ) = -\\frac{1}{2}\\) and \\(\\sin(240^\\circ) = -\\frac{\\sqrt{3}}{2}\\):\n\\[\n\\omega_3 = \\frac{1}{r} ( (v_x \\cos(\\theta) + v_y \\sin(\\theta)) ( -\\frac{1}{2} ) + (-v_x \\sin(\\theta) + v_y \\cos(\\theta)) ( -\\frac{\\sqrt{3}}{2} ) + \\omega_{\\text{body}} h )\n\\]\nExpanding and simplifying:\n\\[\n\\omega_3 = \\frac{1}{r} ( -\\frac{1}{2} v_x \\cos(\\theta) - \\frac{1}{2} v_y \\sin(\\theta) - \\frac{\\sqrt{3}}{2} (-v_x \\sin(\\theta)) - \\frac{\\sqrt{3}}{2} v_y \\cos(\\theta) + \\omega_{\\text{body}} h )\n\\]\n\\[\n\\omega_3 = \\frac{1}{r} ( -\\frac{1}{2} v_x \\cos(\\theta) - \\frac{1}{2} v_y \\sin(\\theta) + \\frac{\\sqrt{3}}{2} v_x \\sin(\\theta) - \\frac{\\sqrt{3}}{2} v_y \\cos(\\theta) + \\omega_{\\text{body}} h )\n\\]\nCombining terms and simplifying further:\n\\[\n\\omega_3 = \\frac{1}{r} ( -\\frac{\\sqrt{3} \\sin(\\theta) v_y}{3} + \\frac{\\cos(\\theta) v_y}{3} - \\frac{\\sqrt{3} \\cos(\\theta) v_x}{3} - \\frac{\\sin(\\theta) v_x}{3} + \\omega_{\\text{body}} h )\n\\]\nFinal Equations\nThe final simplified equations for the angular velocities of the wheels are:\n\\[\n\\omega_1 = \\frac{1}{r} ( v_x \\cos(\\theta) + v_y \\sin(\\theta) + \\omega_{\\text{body}} h )\n\\]\n\\[\n\\omega_2 = \\frac{1}{r} ( \\frac{\\cos(\\theta) v_y}{3} - \\frac{\\sin(\\theta) v_x}{3} + \\frac{\\sqrt{3} \\sin(\\theta) v_y}{3} + \\frac{\\sqrt{3} \\cos(\\theta) v_x}{3}  + \\omega_{\\text{body}} h )\n\\]\n\\[\n\\omega_3 = \\frac{1}{r} ( -\\frac{\\sqrt{3} \\sin(\\theta) v_y}{3} + \\frac{\\cos(\\theta) v_y}{3} - \\frac{\\sqrt{3} \\cos(\\theta) v_x}{3} - \\frac{\\sin(\\theta) v_x}{3} + \\omega_{\\text{body}} h )\n\\]\nThese equations relate the global motion commands (\\(v_x\\), \\(v_y\\), \\(\\omega_{\\text{body}}\\)) to the angular velocities of the three wheels (\\(\\omega_1\\), \\(\\omega_2\\), \\(\\omega_3\\)) taking into account the robot’s orientation \\(\\theta\\), wheel radius \\(r\\) and distance from center to the wheels \\(h\\)."
  },
  {
    "objectID": "projects/posts/bigT_ros_control/index.html#converting-angular-velocities-to-pwm-signal-for-motor-control",
    "href": "projects/posts/bigT_ros_control/index.html#converting-angular-velocities-to-pwm-signal-for-motor-control",
    "title": "ROS Navigation and Control for a Triangular Holonomic Robot",
    "section": "4.5 Converting angular velocities to PWM signal for motor control",
    "text": "4.5 Converting angular velocities to PWM signal for motor control\nAfter we’ve calculated the angular velocities in terms on radians per second (\\(rad/s\\)), we have to convert to a Pulse Width Modulation (PWM) signal to send updated speed and direction to the motors.\n1. Angular Velocity to RPM\nWe first convert angular velocity (\\(rad/s\\)) to revolutions per minute (RPM).\nWe know that \\(1 \\space revolution=2\\pi \\space radians\\) and \\(1 \\space minute = 60 \\space seconds\\), therefore, the conversion factor is:\n\\[\nRPM = \\omega\\times(60/2\\pi) \\approx \\omega\\times 9.5493 \\approx \\omega\\times 9.55\n\\]\n2. Conversion from RPM to PWM\nIn order to measure and calibrate the relationship betwen PWM and RPM for the motor, an empirical experiment was performed in order to collect measurements and fit a linear regression model to the data.\nFor series of PWM signals from a low duty cycle to (eg. 10%) to high duty cycle (eg. 90%) generated using an oscilloscope, the respective RPM for the motor was recorded using a tachometer.\nThe resulting linear relationship is:\n\\[\nPWM = a \\times RPM + b  = 2.4307 \\times RPM + 36.2178\n\\]\nThis means that for every unit increase in RPM, the PWM value increases by approximately 2.4307 units, and when the RPM is zero, the PWM value starts at approximately 36.2178.\n3. Handling Very Small Angular Velocities\nSmall angular velocities &lt;=0.05 are ignored, to smooth noise in the signal and avoid unnecessary motor activation.\n\n\tif(fabs(omega) &lt;= 0.05) return 0;\n\n4. Linear Equation of Angular velocity to PWM\nThrough substituting and simplifying the equations above, we can derive the following linear relationship for converting angular velocity to PWM.\n\\[\nPWM = 2.4307 \\times RPM + 36.2178 =  2.43 \\times (\\omega\\times 9.55) + 36.22\n\\]\nThe result is then returned as an integer PWM value which can be used to control motor speed. We later apply this to each wheel when we calculate the new angular velocities from sensor feedback. We make \\(\\omega\\) absolute \\(|\\omega_{\\text{wheel}}|\\) in order to ensure the angular velocity is never non-negative.\n\nint\tomega2pwm(float omega) {\n\t/*\n\t\tomega ... angular velocity ( in rad/s )\n\t\trpm = omega*9.5493; // conversion from rad/s to rpm\t ( 1/(2*pi)*60 = 9.5493 )\n\t\tpwm = 2.4307*rpm + 36.2178; // conversion of rpm to pwm values\n\t*/\n\tif(fabs(omega) &lt;= 0.05) return 0;\n\n\treturn (int)(2.43*(fabs(omega)*9.55) + 36.22);\n}\n\n5. Setting Direction Bit of Motor\nThe direction bit of the motor is set based on the sign of the angular velocity \\(\\omega_{\\text{wheel}}\\).\n\nint sign(float number){\n\tif(number&gt;=0) return 1; else return 0;\n}\n\n\nThis project was executed as part of the course - ECEN430: Advanced Mechatronic Engineering 2: Intelligence and Design at Victoria University of Wellington 2018."
  },
  {
    "objectID": "blog/posts/mamba/index.html#sec-transformer-limitations",
    "href": "blog/posts/mamba/index.html#sec-transformer-limitations",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "1.1 Limitations of Transformers for Long Contexts",
    "text": "1.1 Limitations of Transformers for Long Contexts\nIn recent years, the transformer architecture has dominated AI, leading to significant advancements in various fields, including solving the protein folding problem (AlphaFold), performing in the 80-90th percentile in the uniform bar exams and college-level AP subjects[7], to translating between nuanced languages from Tamil, Turkish, Arabic to Urdu. However, transformers face challenges with long sequences (e.g., 100,000 tokens or more) due to the quadratic complexity of the self-attention mechanism, which results in substantial computational and memory costs especially during inference due the \\(N^2\\) size of the \\((QK)V\\) matrix.\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V\n\\tag{1.1}\\]\nA useful visual explainer can be seen with the BERTViz tool [8] (see Figure 1.5) where we can observe attention for one or more attention heads in the same layer as well as how individual neurons in the query and key vectors are activated in the attention computation.\n\n\n\n\n\n\n\n\nHead View: Visualising attention head activations between different layers. Connecting lines are weighted based on the attention score between respective words.\n\n\n\n\n\n\n\nNeuron View: Visualising query, key and value embeddings when computing attention between each token and other tokens within the sequence. Positive values are colored as blue and negative values as orange.\n\n\n\n\n\n\nFigure 1.5: Visualising Attention Weights in Transformer Networks [8]\n\n\n\nWe can observe in the following experiments in Figure 1.6 that GPT4’s recall performance starts to degrade above 73K tokens where we observce low recall performance when fact is placed between 7-50% document depth. However, facts at the beginning of documents were recalled regardless of document length. This also seems to be the case for Anthropic’s Claude 2.1 model.\n\n\n\n\n\n\n\n\nOpenAI’s GPT-4-128K Long Context Performance\n\n\n\n\n\n\n\nAnthropic’s Claude 2.1 Long Context Performance\n\n\n\n\n\n\nFigure 1.6: Needle In A Haystack: Pressure Testing LLMs Results for Long Context Retrieval [9]\n\n\n\nTransformers suffer from the “lost in the middle” issue with long contexts where the model struggles to retrieve the answer if the context is in the middle of the document. This is mitigated by prompt compression techniques which involve training smaller prompt compression LLMs to identify and remove non-essential tokens before feeding them to the larger LLM and retrieval-augmented-retrieval (RAG) techniques which involve adding additional context to prompts to allow LLMs to operate on facts fetched from external sources outside of LLM’s trained context. However, this involves relying on techniques and architectures to augment the input and output to the model as opposed to improving long context performance of the model itself.\n\n\n\n\n\n\nFigure 1.7: Lost in the Middle: Performance Degrades When Information Access is in the Middle of Document [10]\n\n\n\n\n1.1.1 Limitations of the the KV Cache\nThe KV cache is the caching of each key and value tensor of size d_head for each attention heads of each layer for each token in a batched sequence to enable the self-attention mechanism to scale linearly instead of quadratically. The precise space required by each tensor parameter will depend on the precision \\(p_{a}\\) (eg. 4bytes/parameter for full precision float32, 2 bytes/parameter for half-precision float16, 1 bytes/parameter int8) [11]. This can be expressed as:\n\\[\n2 \\cdot BS \\cdot T \\cdot n_{layers} \\cdot n_{heads} \\cdot d_{head} \\cdot p_{a}\n\\tag{1.2}\\]\n\n\n\n\n\n\nFigure 1.8: Comparison of scaled dot-product attention with and without KV caching [12]\n\n\n\nThe challenge with the KV cache is that it will grow linearly with the sequence length and batch size. Since sequence length is unknown prior, the KV cache size can consume an unbounded amount of GPU memory in the order of ~1MB/token and can easily grow larger than the model weights if implemented naively. This is not to mention the amount of data transfer required to transfer the model and KV cache at scale. If we can reduce the GPU memory requirement to allow for more compute space, latency can be greatly improved.\nThere has been a lot of recent advancement in techniques and significant engineering efforts to reduce the KV cache for ever-growing context size. However, if we can try to solve this at the modelling level, it will greatly reduce the system complexity require to scale AI adoption in future. These techniques include [13]:\n\nnovel attention architectures to reduce the number of attention heads\ncache compression strategies to more intelligently prioritise a fixed KV cache eg. caching the very first positional tokens (”sink tokens”) and the last neighboring tokens (local attention)\nefficient memory management to share the cache across different requests on the host especially for common tokens\nquantising the model weights and activations to reduce the GPU memory footprint\nstorage capacity expansion such as offloading memory to CPU or single and multi-host model parallelism, a technique to pool memory over multiple devices by sharding the model over multiple GPUs used often when LLM cannot fit on single GPU in training\n\n\\(N\\): Sequence length \\(d\\): Model parameters\n\n\n\nPros\nCons\n\n\n\n\nUnreasonably effective at modelling complex dependencies: Each token explicitly attends to all other tokens in the sequence. Unlike architectures that rely on a fixed-sized state as a summary, masked attention in transformers enables each token to see an uncompressed view of the sequence during training.\nQuadratic scaling with context length: Since every input attends to all prior inputs, the total amount of computation increases quadratically both in time and space - \\(O(N^2d)\\). The cost of inference is therefore quadratic in nature, having to recalculate attention for the full sequence. However, this can be reduced to space and time complexity to \\(\\approx O(Nd)\\) with a KV cache[14].\n\n\nHighly parallel training: There are no dependencies along the time dimension, and the core operations are matrix multiplications, which hardware accelerators have been excellent at parallelisation for decades.\nWeak inductive bias: Unlike CNNs, there is almost no prior knowledge of dependency patterns. For example, position information only comes from absolute/relative positional embeddings."
  },
  {
    "objectID": "blog/posts/mamba/index.html#kernel-fusion",
    "href": "blog/posts/mamba/index.html#kernel-fusion",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "3.2 Kernel Fusion",
    "text": "3.2 Kernel Fusion\nOne of the biggest speedups is from implementing the parallel associative scan step as a single GPU kernel operation through GPU kernel fusion. The discretisation, parallel associative scan operation and multiplication with \\(\\mathbf{C}\\) are performed in the SRAM before writing results back to HBM. Therefore, we can save a lot of time by creating a custom kernel to fuse the operations to reduce the IO between SRAM and HBM by factor of \\(O(D)\\) - the state dimension [3].\n\n\n\n\n\n\n\n\n\n\n(Left): Average Memory Bandwidth for A100 (Center): FlashAttention Architecture (Right): Speedup of Attention on GPT-2\n\n\n\n\nFigure 3.5: Example of Kernel Fusion Enabling Efficient Operations in FlashAttention [19]"
  },
  {
    "objectID": "blog/posts/mamba/index.html#please-feel-free-to-suggest-any-improvements-or-corrections-if-need.-thanks-for-reading-and-hope-you-learnt-something-useful-from-my-musings",
    "href": "blog/posts/mamba/index.html#please-feel-free-to-suggest-any-improvements-or-corrections-if-need.-thanks-for-reading-and-hope-you-learnt-something-useful-from-my-musings",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "3.2 Please feel free to suggest any improvements or corrections if need. Thanks for reading and hope you learnt something useful from my musings! :)",
    "text": "3.2 Please feel free to suggest any improvements or corrections if need. Thanks for reading and hope you learnt something useful from my musings! :)"
  },
  {
    "objectID": "blog/posts/mamba/index.html#sec-kv-cache",
    "href": "blog/posts/mamba/index.html#sec-kv-cache",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "1.2 The KV Cache",
    "text": "1.2 The KV Cache\nMore croncretely than approximating the model parameters as \\(d\\), the KV cache is the caching of each key and value tensor of size \\(d_head\\) for each token in the sequence where the space required by each tensor parameter will depend on the precision (eg. 4bytes/parameter for full precision float32, 2 bytes/parameter for half-precision float16, etc). The per-token memory consumption\n\n\n\n\n\n\nFigure 1.7: Comparison of scaled dot-product attention with and without KV caching. emb_size means embedding size. [11]"
  },
  {
    "objectID": "blog/posts/mamba/index.html#parallel-scan",
    "href": "blog/posts/mamba/index.html#parallel-scan",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "3.1 Parallel Scan",
    "text": "3.1 Parallel Scan\nThe parallel scan enables us to"
  },
  {
    "objectID": "blog/posts/mamba/index.html#recomputation",
    "href": "blog/posts/mamba/index.html#recomputation",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "3.3 Recomputation",
    "text": "3.3 Recomputation\nThe memory and compute requirement is further optimised by re-computing cheap operations instead of saving and reading intermediate states between stages in the entire selective SSM block (input projection, convolution, activation, scan, output projection). For instance, re-computing intermediate states (\\(\\mathbf{\\Delta}\\), \\(\\mathbf{\\bar{A}}\\), \\(\\mathbf{\\bar{B}}\\), \\(\\mathbf{\\bar{C}}\\)) to compute gradients on the backward pass vs reading them from memory from the forward pass.\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network Computation Graph Source\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecomputing of Activations on Backward Pass: Blue = forward, Red = backward Source\n\n\n\n\n\n\n\n\n\n\n\n\n\nSaving GPU Memory with Re-computation [21]\n\n\n\n\n\n\nFigure 3.6: Recomputation Explained: Example of Recomputing Intermediate States for Backward Pass\n\n\n\n\nPlease feel free to suggest any improvements or corrections. Thanks for reading and hope you learnt something useful from my journey! :)"
  },
  {
    "objectID": "blog/posts/mamba/index.html#parallel-scan-and-kernel-fusion",
    "href": "blog/posts/mamba/index.html#parallel-scan-and-kernel-fusion",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "3.1 Parallel Scan and Kernel Fusion",
    "text": "3.1 Parallel Scan and Kernel Fusion\nOne of the biggest speedups is from implementing the parallel associative scan step performed at each timestep as a single GPU kernel operation through GPU kernel fusion. Therefore, the discretisation, parallel associative scan operation in preparing \\(\\mathbf{\\bar{A}}\\), \\(\\mathbf{\\bar{B}}\\) and multiplication with \\(\\mathbf{C}\\) are performed in the SRAM before writing results back to HBM. Therefore, we can save a lot of time by creating a custom kernel to fuse the operations to reduce the memory load to one and IO between SRAM and HBM by factor of \\(O(D)\\) - the state dimension.\n\n\n\n\n\n\n\n\n\n\n(Left): Average Memory Bandwidth for A100 (Center): FlashAttention Architecture (Right): Speedup of Attention on GPT-2\n\n\n\n\nFigure 3.4: Example of Kernel Fusion Enabling Efficient Operations in FlashAttention [19]"
  },
  {
    "objectID": "blog/posts/mamba/index.html#section",
    "href": "blog/posts/mamba/index.html#section",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "3.2 ",
    "text": "3.2 \nGPUs execute operations in units of “kernels”, therefore, kernel fusion is simply to fuse two kernels into one where the primary use is to re-use loads into memory and reduce redundant loads and stores. For a single tensor operation without fusion, a tensor is often loaded in fast memory (SRAM), performs the operation and then saves it back to high-bandwidth memory (HBM) of the GPU.\nTherefore, we can save a lot of time by creating a custom kernel to fuse the operations to reduce the memory load to one and IO between SRAM and HBM by factor of \\(O(D)\\) - the state dimension.\n\n\n\n\n\n\n\n\n\n\n(Left): Average Memory Bandwidth for A100 (Center): FlashAttention Architecture (Right): Speedup of Attention on GPT-2\n\n\n\n\nFigure 3.4: Example of Kernel Fusion Enabling Efficient Operations in FlashAttention [19]"
  },
  {
    "objectID": "blog/posts/mamba/index.html#parallel-associative-scan",
    "href": "blog/posts/mamba/index.html#parallel-associative-scan",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "3.1 Parallel Associative Scan",
    "text": "3.1 Parallel Associative Scan\nDespite not being able to parallelise the state computation with convolution, we can speed up the recurrent computation with the parallel associative scan, otherwise known as the prefix sum (scan) problem. The parallel prefix scan algorithm is also called the Blelloch Algorithm named after it’s author. The recurrent formula of the SSM model can also be thought of as a scan operation where each state is the sum of the previous state and the current input. To generate the output, we multiply each \\(h_k\\) with \\(C\\) to generate \\(y_k\\). The parallel scan algorithm is based on the associative property where \\(A * B * C = (A * B) * C = A * (B * C)\\) which states that the order of the operations does not matter. The time complexity instead of being \\(O(N)\\) is reduced to \\(O(N/th)\\) where \\(th\\) is the number of parallel threads on GPU.\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation of Linear Scan [20]\n\n\n\n\n\n\n\n\n\n\n\nVisualisation of Blelloch Algorithm (Work-Efficient Parallel Prefix Scan) [20]\n\n\n\n\n\n\nFigure 3.4: Visualising the Linear vs Parallel Associative Scan Operation\n\n\n\nWhen the sequence length \\(T\\) is too long to fit the full sequence into SRAM which is much smaller than HBM, the sequences are split into chunks where the fused scan is performed on each chunk."
  },
  {
    "objectID": "blog/posts/mamba/index.html#recomputatio",
    "href": "blog/posts/mamba/index.html#recomputatio",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "3.3 Recomputatio",
    "text": "3.3 Recomputatio\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network Computation Graph Source\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecomputing of Activations on Backward Pass: Blue = forward, Red = backward Source\n\n\n\n\n\n\n\n\n\n\n\n\n\nSaving GPU Memory with Re-computation [21]\n\n\n\n\n\n\nFigure 3.6: Recomputation Explained: Example of Recomputing Intermediate States for Backward Pass\n\n\n\n\nPlease feel free to suggest any improvements or corrections. Thanks for reading and hope you learnt something useful from my journey! :)"
  },
  {
    "objectID": "blog/posts/mamba/index.html#selective-ssm-for-context-aware-reasoning",
    "href": "blog/posts/mamba/index.html#selective-ssm-for-context-aware-reasoning",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "3.1 Selective SSM for Context Aware Reasoning",
    "text": "3.1 Selective SSM for Context Aware Reasoning\nA model’s ability to perform in-context reasoning can be inferred from their performance on the tasks of selective copying and inductive reasoning [3]. Selective copying refers to the model’s ability to identify and reproduce specific phrases, entities or patterns in the input, and incorporate it appropriately in the generated output and is a task to test a model’s memorisation capabilities. Induction heads is an associative recall task to test a model’s ablility to perform inductive reasoning based on observed patterns, and learned underlying concepts and relationships.\n\n\n\n\n\n\n\n\n\n\n\n\nSelective Copying: This requires time-varying models that can selectively remember or ignore inputs depending on their content.\n\n\n\n\n\n\n\n\n\n\n\nInduction Heads: This is an associative recall task which requires retrieving an answer based on context, a key ability of LLMs.\n\n\n\n\n\n\nFigure 3.2: Tasks to Demontrastae Context-Aware Reasoning [1]\n\n\n\nThis introduction of selection enables Mamba to perform:\n\nover 2x as well than S4 and predecessor models on the selective copying task reaching accuracy over 97%.\n~100% accuracy on inductive heads due to ability to selectively remember the relevant token while ignoring everything else in between. It is able to generalise to million-length sequences, 4000x longer seen during training, whilst other methods such as multi-head attention (MHA) variants fail go perform with 2x sequence lengt\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelective Copying Results: Accuracy for combinations of architectures\n\n\n\n\n\n\n\n\n\n\n\nInduction Heads Extrapolation: Mamba has ability to maintain high induction test accuracy for sequence length up to 1 million tokens\n\n\n\n\n\n\nFigure 3.3: Mamba Performance on Context-Aware Reasoning Tasks [3]"
  },
  {
    "objectID": "blog/posts/mamba/index.html#selective-ssm-for-parallelised-training",
    "href": "blog/posts/mamba/index.html#selective-ssm-for-parallelised-training",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "3.2 Selective SSM for Parallelised Training",
    "text": "3.2 Selective SSM for Parallelised Training\nHowever, making the system time-invariant means we can no longer perform convolution in Equation 2.4 to parallelise training since it assumes a fixed kernel. To address this, Mamba introduces the selective scan layer. It is the implementation of a hard-aware selective parallel scan algorithm with the same GPU kernel fusion techniques in FlashAttention [19] for transformers, as a result of Mamba being a collaborative paper between Albert Gu (S4) and Tri Dao (FlashAttention). Therefore, the core optimisations for all three techniques, parallel scan, kernel fusion and recomputation in the selective SSM layer are to try and perform as many operations in the fast memory (SRAM) layer of the GPU before saving results back to high-bandwidth memory (HBM) of the GPU (see Figure 3.5) to reduce the data transfer (IO) between them.\n\n\n\n\n\n\n\n\n\n\n\n\nSelective SSM Architecture Simplified: The select state layer is kept and computed in SRAM. [1]\n\n\n\n\n\n\n\n\n\n\n\n\n\nState Selection with Hardware-Aware State Expansion: The selection mechanism ensures the expanded matrix states only materialise in SRAM to reduce data transfer and computation between SRAM&lt;&gt;HBM. [3]\n\n\n\n\n\n\nFigure 3.3: Mamba: Selective SSM Architecture\n\n\n\n\n3.2.1 Parallel Associative Scan\nDespite not being able to parallelise the state computation with convolution, we can speed up the recurrent computation with the parallel associative scan, otherwise known as the prefix sum (scan) problem. The parallel prefix scan algorithm is also called the Blelloch Algorithm named after it’s author. The recurrent formula of the SSM model can also be thought of as a scan operation where each state is the sum of the previous state and the current input. To generate the output, we multiply each \\(h_k\\) with \\(C\\) to generate \\(y_k\\). The parallel scan algorithm is based on the associative property where \\(A * B * C = (A * B) * C = A * (B * C)\\) which states that the order of the operations does not matter. The time complexity instead of being \\(O(N)\\) is reduced to \\(O(N/pt)\\) where \\(pt\\) is the number of parallel threads on GPU.\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation of Linear Scan\n\n\n\n\n\n\n\n\n\n\n\nVisualisation of Blelloch Algorithm (Work-Efficient Parallel Prefix Scan)\n\n\n\n\n\n\nFigure 3.4: Visualising the Linear vs Parallel Associative Scan Operation [20]\n\n\n\nWhen the sequence length \\(T\\) is too long to fit the full sequence into SRAM which is much smaller than HBM, the sequences are split into chunks where the fused scan is performed on each chunk.\n\n\n3.2.2 Kernel Fusion\nOne of the biggest speedups is from implementing the parallel associative scan step as a single GPU kernel operation through GPU kernel fusion. The discretisation, parallel associative scan operation and multiplication with \\(\\mathbf{C}\\) are performed in the SRAM before writing results back to HBM. Therefore, we can save a lot of time by creating a custom kernel to fuse the operations to reduce the IO between SRAM and HBM by factor of \\(O(D)\\) - the state dimension [3].\n\n\n\n\n\n\n\n\n\n\n(Left): Average Memory Bandwidth for A100 (Center): FlashAttention Architecture (Right): Speedup of Attention on GPT-2\n\n\n\n\nFigure 3.5: Example of Kernel Fusion Enabling Efficient Operations in FlashAttention [19]\n\n\n\n\n\n3.2.3 Recomputation\nThe memory and compute requirement is further optimised by re-computing cheap operations instead of saving and reading intermediate states between stages in the entire selective SSM block (input projection, convolution, activation, scan, output projection). For instance, re-computing intermediate states (\\(\\mathbf{\\Delta}\\), \\(\\mathbf{\\bar{A}}\\), \\(\\mathbf{\\bar{B}}\\), \\(\\mathbf{\\bar{C}}\\)) to compute gradients on the backward pass vs reading them from memory from the forward pass.\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network Computation Graph Source\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecomputing of Activations on Backward Pass: Blue = forward, Red = backward Source\n\n\n\n\n\n\n\n\n\n\n\n\n\nSaving GPU Memory with Re-computation [21]\n\n\n\n\n\n\nFigure 3.6: Example of Recomputing Intermediate States for Backward Pass\n\n\n\n\nPlease feel free to suggest any improvements or corrections. Thanks for reading and hope you learnt something useful from my journey! :)"
  },
  {
    "objectID": "blog/posts/mamba/index.html#selective-ssm-layer-for-parallelised-training",
    "href": "blog/posts/mamba/index.html#selective-ssm-layer-for-parallelised-training",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "3.2 Selective SSM Layer for Parallelised Training",
    "text": "3.2 Selective SSM Layer for Parallelised Training\nHowever, making the system time-varying means we can no longer perform convolution in Equation 2.4 to parallelise training since it assumes a fixed kernel. To address this, Mamba introduces the selective scan layer. It is the implementation of a hard-aware selective parallel scan algorithm with the same GPU kernel fusion techniques in FlashAttention [21] for transformers, as a result of Mamba being a collaborative paper between Albert Gu (S4) and Tri Dao (FlashAttention). Therefore, the core optimisations for all three techniques, parallel scan, kernel fusion and recomputation in the selective SSM layer are to try and perform as many operations in the fast memory (SRAM) layer of the GPU before saving results back to high-bandwidth memory (HBM) (see Figure 3.6). This reduces the data transfer (IO) between them, as loading is often the slowest process [22]. For more details on model optimisation on GPUs, this is a good read from first principles.\n\n\n\n\n\n\n\n\n\n\n\n\n(Left): Average Memory Bandwidth for A100 (Right): Selective SSM Architecture Simplified: The select state layer is kept and computed in SRAM. [1]\n\n\n\n\n\n\n\n\n\n\n\n\n\nState Selection with Hardware-Aware State Expansion: The selection mechanism ensures the expanded matrix states only materialise in SRAM to reduce data transfer and computation between SRAM&lt;&gt;HBM. [3]\n\n\n\n\n\n\nFigure 3.4: Mamba: Selective SSM Architecture\n\n\n\n\n3.2.1 Parallel Associative Scan\nDespite not being able to parallelise the state computation with convolution, we can speed up the recurrent computation with the parallel associative scan, otherwise known as the parallel prefix sum (scan) problem. The work-efficient parallel prefix scan algorithm is also known as the Blelloch Algorithm named after it’s author. The recurrent formula of the SSM model can also be thought of as a scan operation where each state is the sum of the previous state and the current input. To generate the output, we multiply each \\(h_k\\) with \\(C\\) to generate \\(y_k\\). The parallel scan algorithm is based on the associative property where \\(A * B * C = (A * B) * C = A * (B * C)\\) which states that the order of the operations does not matter therefore reducing time complexity from \\(O(N)\\) to \\(O(N/pt)\\) where \\(pt\\) is the number of parallel threads on GPU. See here for more implementation details of the parallel scan operation and deeper understanding of the binary associative operator in parallelising computation of \\(h_k = \\mathbf{\\bar{A}}h_{k-1} + \\mathbf{\\bar{B}}x_k\\).\n\n\n\n\n\n\n\n\n\n\n\n\nVisualisation of Linear Scan\n\n\n\n\n\n\n\n\n\n\n\nVisualisation of Blelloch Algorithm (Work-Efficient Parallel Prefix Scan)\n\n\n\n\n\n\nFigure 3.5: Visualising the Linear vs Parallel Associative Scan Operation [23]\n\n\n\n\n\n3.2.2 Kernel Fusion\nOne of the biggest efficiency gains is from implementing the parallel associative scan as a single GPU kernel operation through GPU kernel fusion. The discretisation, parallel associative scan operation and multiplication with \\(\\mathbf{C}\\) are performed in the SRAM before writing results back to HBM. Therefore, a lot of time is saved by creating a custom kernel to fuse the operations required to perform the scan operation into a single layer to reduce the IO between SRAM and HBM by factor of \\(O(D)\\) - the state dimension [3]. When the sequence length \\(T\\) is too long to fit the full sequence into SRAM which is much smaller than HBM, the sequences are split into chunks where the fused scan is performed on each chunk.\n\n\n\n\n\n\n\n\n\n\n(Left): FlashAttention: The \\(\\mathbf{(QK)V}\\) matrix of size \\(N^2\\) is computed in SRAM using tiling before being written to HBM. (Right): Speedup of Attention on GPT-2\n\n\n\n\nFigure 3.6: Example of Kernel Fusion Enabling Efficient Operations in FlashAttention [21]\n\n\n\n\n\n3.2.3 Recomputation\nThe memory and compute requirement is further optimised by re-computing cheap operations instead of saving and reading intermediate states between stages in the entire selective SSM block (input projection, convolution, activation, scan, output projection). For instance, re-computing intermediate states (\\(\\mathbf{\\Delta}\\), \\(\\mathbf{\\bar{A}}\\), \\(\\mathbf{\\bar{B}}\\), \\(\\mathbf{\\bar{C}}\\)) to compute gradients on the backward pass vs reading them from HBM memory from the forward pass.\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network Computation Graph Source\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecomputing of Activations on Backward Pass: Blue = forward, Red = backward Source\n\n\n\n\n\n\n\n\n\n\n\n\n\nSaving GPU Memory with Re-computation [24]\n\n\n\n\n\n\nFigure 3.7: Example of Recomputing Intermediate States for Backward Pass"
  },
  {
    "objectID": "blog/posts/mamba/index.html#com",
    "href": "blog/posts/mamba/index.html#com",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "4.1 Com[]",
    "text": "4.1 Com[]\n\nPlease feel free to suggest any improvements or corrections. Thanks for reading and hope you learnt something useful from my journey! :)"
  },
  {
    "objectID": "blog/posts/mamba/index.html#mamba-architecture",
    "href": "blog/posts/mamba/index.html#mamba-architecture",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "3.3 Mamba Architecture",
    "text": "3.3 Mamba Architecture\nThe Mamba model is made by stacking multiple layers of Mamba blocks, similar to self-attention in the transformer. It is heavily inspired by its predecessor, the Hungry Hungry Hippo (H3) Architecture [25]. It starts with projecting inputs to hidden state, followed by convolution over projected dimensions with sigmoid-weighted linear unit (SILU) /Swish activation [26]. The SSM operation is then computed followed by the skip connection operation \\(\\mathbf{D}\\) before downscaling for another linear projection.\nThe full architecture includes tokenising inputs to an embedding later, followed by the Mamba block repeated N times for the length of the sequence N with the inclusion of couple RMS Norm normalisation layers and a softmax layer for choosing the next output token.\n\n\n\n\n\n\n\n\n\n\n\n\nFrom H3 to the Mamba Block [25]\n\n\n\n\n\n\n\n\n\n\n\nMamba Block Decoder Architecture [1]\n\n\n\n\n\n\nFigure 3.8: Mamba Architecture"
  },
  {
    "objectID": "blog/posts/mamba/index.html#sec-ssm-context-aware",
    "href": "blog/posts/mamba/index.html#sec-ssm-context-aware",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "3.1 Selective SSM for Context Aware Reasoning",
    "text": "3.1 Selective SSM for Context Aware Reasoning\nA model’s ability to perform in-context reasoning can be inferred from their performance on the tasks of selective copying and inductive reasoning [3]. Selective copying refers to the model’s ability to identify and reproduce specific phrases, entities or patterns in the input, and incorporate it appropriately in the generated output and is a task to test a model’s memorisation capabilities. Induction heads is an associative recall task to test a model’s ablility to perform inductive reasoning based on observed patterns, and learned underlying concepts and relationships.\n\n\n\n\n\n\n\n\n\n\n\n\nSelective Copying: This requires time-varying models that can selectively remember or ignore inputs depending on their content.\n\n\n\n\n\n\n\n\n\n\n\nInduction Heads: This is an associative recall task which requires retrieving an answer based on context, a key ability of LLMs.\n\n\n\n\n\n\nFigure 3.2: Tasks to Demonstrate Context-Aware Reasoning [1]\n\n\n\nThis introduction of selection enables Mamba to perform:\n\nover 2x as well than S4 and predecessor models on the selective copying task reaching accuracy over 97%.\n~100% accuracy on inductive heads due to ability to selectively remember the relevant token while ignoring everything else in between. It is able to generalise to million-length sequences, 4000x longer seen during training, whilst other methods such as multi-head attention (MHA) variants fail to perform at 2x sequence length.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelective Copying Results: Accuracy for combinations of architectures\n\n\n\n\n\n\n\n\n\n\n\nInduction Heads Extrapolation: Mamba has ability to maintain high induction test accuracy for sequence length up to 1 million tokens\n\n\n\n\n\n\nFigure 3.3: Mamba Performance on Context-Aware Reasoning Tasks [3]"
  },
  {
    "objectID": "blog/posts/mamba/index.html#mamba-vs-llms-performance-for-language-modelling",
    "href": "blog/posts/mamba/index.html#mamba-vs-llms-performance-for-language-modelling",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "3.4 Mamba vs LLMs Performance for Language Modelling",
    "text": "3.4 Mamba vs LLMs Performance for Language Modelling\nMamba has shown extremely promising results compared to transformer LLM models, where it has been shown that it has been able to perform competitively on commonsense reasoning benchmarks with models twice their size (eg. Mamba 2.8B vs Mixtral 7B). However, large-scale research on Mamba vs transformer models of the same size is yet to be conducted. Their prevalence is yet to be seen given the industry’s investment into productionisation of transformer-based architectures.\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of Mamba variants with different popular 7B LLMs on Piqa, Winogrande, Lambada, and Hellaswag Source\n\n\n\n\n\n\n\n\n\n\n\nEvaluation Comparison of Mamba variants with several similar-sized LLMs [3]\n\n\n\n\n\n\nFigure 3.9: Mamba vs LLMs Performance on Commonsense Reasoning Benchmarks"
  },
  {
    "objectID": "blog/posts/mamba/index.html#for-language",
    "href": "blog/posts/mamba/index.html#for-language",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "4.1 For Language",
    "text": "4.1 For Language\nThe structure of prompting LLMs could go from the 4 steps of system prompt, preamble, few-shot examples to answer to just 2 steps of given an input state with combined context, perform an instruction/answer."
  },
  {
    "objectID": "blog/posts/mamba/index.html#for-vision",
    "href": "blog/posts/mamba/index.html#for-vision",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "4.2 For Vision",
    "text": "4.2 For Vision\n\nPlease feel free to suggest any improvements or corrections. Thanks for reading and hope you learnt something useful from my journey! :)"
  },
  {
    "objectID": "blog/posts/mamba/index.html#applications-and-architectures",
    "href": "blog/posts/mamba/index.html#applications-and-architectures",
    "title": "Structured State Space Sequence Models (S4) and Mamba Explained: A Primer",
    "section": "4.1 Applications and Architectures",
    "text": "4.1 Applications and Architectures\nFrom a recent survey, there are still stability challenges scaling SSMs to the same network size as SoTA transformers especially in vision [27]. Fusion techniques may fill in each others’ shortcomings between CNNs, vision transformers and vision mamba models in future to allow for better generalisation performance with long-context dependencies. For example, this has lead to the open-source release of a new LLM foundation model, Jamba, from AI32 Labs fusing the Transformer, Mamba, and MoE (Mixture-of-Experts) architectures to enable context length of 256K tokens with performance reaching Mixtral-7B and Llama2-7B with a reduced KV cache memory footprint of only 4GB [29].\nThe plethora of Mamba vision variants of late extend the selective scan algorithm to 2 dimensions where the scan techniques can be categorised into four groups: scan mode, scan axis, scan continuity and scan sampling (see Figure 4.2).\nHowever, a recent paper, MambaOut, highlights that Mamba models may not be needed for tasks that do not require long-sequence dependencies and autoregressive characteristics, such as image classification [30] which they prove by showing that MambaOut can outperform SoTA vision Mamba models on ImageNet-1K classification without the Mamba block. It will be fruitful, however, to evaluate Mamba’s performance on detection and segmentation in long-context settings such as with long-term video sequences (movies) or high-dimensional imagery (remote sensing).\nModifying Mamba’s inherent 1D nature of selective scan meant for a causal sequential stream to a bi-directional 2D scan technique has posed algorithmic challenges in scalability and stability, as well as maintaining spatial information without redundancy in computation. Therefore, there needs to be advancements in the scanning operators in order to apply Mamba on higher-dimensional non-causal visual data more effectively in future and to capture and obtain more comprehensive skewed feature representations to enhance the feature learning in SSMs.\n\n\n\n\n\n\n\n\n\n\n\n\nVision Mamba Scan Techniques\n\n\n\n\n\n\n\n\n\n\n\nVision Mamba Model Landscape\n\n\n\n\n\n\nFigure 4.2: Vision Mamba Survey [31]\n\n\n\n\nPlease feel free to suggest any improvements or corrections. Thanks for reading and hope you learnt something useful from my journey! :)"
  }
]